<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" /><title>The vinum Volume Manager</title><link rel="stylesheet" type="text/css" href="docbook.css" /><link rev="made" href="mailto:doc@FreeBSD.org" /><meta name="generator" content="DocBook XSL Stylesheets V1.79.1" /><script xmlns="" type="text/javascript" src="/layout/js/google.js"></script></head><body><div xml:lang="en" class="article" lang="en"><div xmlns="" class="titlepage"><div><div><h1 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp44016760"></a>The <code class="filename">vinum</code> Volume Manager</h1></div><div><div xmlns="http://www.w3.org/1999/xhtml" class="authorgroup"><div class="author"><h3 class="author"><span class="firstname">Greg</span> <span class="surname">Lehey</span></h3><span class="contrib">Originally written by </span> </div></div></div></div><div class="docformatnavi">
      [
      <a href="index.html">Split HTML</a>
      /
      
	  Single HTML
	
      ]
    </div><hr /></div><div class="toc"><div class="toc-title">Table of Contents</div><dl class="toc"><dt><span class="sect1"><a href="#vinum-synopsis">1. Synopsis</a></span></dt><dt><span class="sect1"><a href="#vinum-access-bottlenecks">2. Access Bottlenecks</a></span></dt><dt><span class="sect1"><a href="#vinum-data-integrity">3. Data Integrity</a></span></dt><dt><span class="sect1"><a href="#vinum-objects">4. <code class="filename">vinum</code> Objects</a></span></dt><dt><span class="sect1"><a href="#vinum-examples">5. Some Examples</a></span></dt><dt><span class="sect1"><a href="#vinum-object-naming">6. Object Naming</a></span></dt><dt><span class="sect1"><a href="#vinum-config">7. Configuring <code class="filename">vinum</code></a></span></dt><dt><span class="sect1"><a href="#vinum-root">8. Using <code class="filename">vinum</code> for the Root
	File System</a></span></dt></dl></div><div class="sect1"><div xmlns="" class="titlepage"><div><div><h2 xmlns="http://www.w3.org/1999/xhtml" class="title" style="clear: both"><a id="vinum-synopsis"></a>1. Synopsis</h2></div></div></div><p>No matter the type of disks, there are always potential
      problems.  The disks can be too small, too slow, or too
      unreliable to meet the system's requirements.  While disks are
      getting bigger, so are data storage requirements.  Often a file
      system is needed that is bigger than a disk's capacity.  Various
      solutions to these problems have been proposed and
      implemented.</p><p>One method is through the use of multiple, and sometimes
      redundant, disks.  In addition to supporting various cards and
      controllers for hardware Redundant Array of Independent
      Disks <acronym class="acronym">RAID</acronym> systems, the base FreeBSD system
      includes the <code class="filename">vinum</code> volume manager, a
      block device driver that implements virtual disk drives and
      addresses these three problems.  <code class="filename">vinum</code>
      provides more flexibility, performance, and reliability than
      traditional disk storage and implements
      <acronym class="acronym">RAID</acronym>-0, <acronym class="acronym">RAID</acronym>-1, and
      <acronym class="acronym">RAID</acronym>-5 models, both individually and in
      combination.</p><p>This chapter provides an overview of potential problems with
      traditional disk storage, and an introduction to the
      <code class="filename">vinum</code> volume manager.</p><div xmlns="" class="note"><h3 class="admontitle">Note: </h3><p xmlns="http://www.w3.org/1999/xhtml">Starting with FreeBSD 5, <code class="filename">vinum</code>
	has been rewritten in order to fit into the <a class="link" href="../../../../doc/en_US.ISO8859-1/books/handbook/geom.html" target="_top">GEOM architecture</a>, while retaining the
	original ideas, terminology, and on-disk metadata.  This
	rewrite is called <span class="emphasis"><em>gvinum</em></span> (for <span class="emphasis"><em>
	  GEOM vinum</em></span>).  While this chapter uses the term
	<code class="filename">vinum</code>, any command invocations should
	be performed with <code class="command">gvinum</code>.  The name of the
	kernel module has changed from the original
	<code class="filename">vinum.ko</code> to
	<code class="filename">geom_vinum.ko</code>, and all device nodes
	reside under <code class="filename">/dev/gvinum</code> instead of
	<code class="filename">/dev/vinum</code>.  As of
	FreeBSD 6, the original <code class="filename">vinum</code>
	implementation is no longer available in the code base.</p></div></div><div class="sect1"><div xmlns="" class="titlepage"><div><div><h2 xmlns="http://www.w3.org/1999/xhtml" class="title" style="clear: both"><a id="vinum-access-bottlenecks"></a>2. Access Bottlenecks</h2></div></div></div><p>Modern systems frequently need to access data in a highly
      concurrent manner.  For example, large FTP or HTTP servers can
      maintain thousands of concurrent sessions and have multiple
      100 Mbit/s connections to the outside world, well beyond
      the sustained transfer rate of most disks.</p><p>Current disk drives can transfer data sequentially at up to
      70 MB/s, but this value is of little importance in an
      environment where many independent processes access a drive, and
      where they may achieve only a fraction of these values.  In such
      cases, it is more interesting to view the problem from the
      viewpoint of the disk subsystem.  The important parameter is the
      load that a transfer places on the subsystem, or the time for
      which a transfer occupies the drives involved in the
      transfer.</p><p>In any disk transfer, the drive must first position the
      heads, wait for the first sector to pass under the read head,
      and then perform the transfer.  These actions can be considered
      to be atomic as it does not make any sense to interrupt
      them.</p><p><a id="vinum-latency"></a> Consider a typical transfer of
      about 10 kB: the current generation of high-performance
      disks can position the heads in an average of 3.5 ms.  The
      fastest drives spin at 15,000 rpm, so the average
      rotational latency (half a revolution) is 2 ms.  At
      70 MB/s, the transfer itself takes about 150 &#956;s,
      almost nothing compared to the positioning time.  In such a
      case, the effective transfer rate drops to a little over
      1 MB/s and is clearly highly dependent on the transfer
      size.</p><p>The traditional and obvious solution to this bottleneck is
      <span class="quote">&#8220;<span class="quote">more spindles</span>&#8221;</span>:  rather than using one large disk,
      use several smaller disks with the same aggregate storage
      space.  Each disk is capable of positioning and transferring
      independently, so the effective throughput increases by a factor
      close to the number of disks used.</p><p>The actual throughput improvement is smaller than the
      number of disks involved.  Although each drive is capable of
      transferring in parallel, there is no way to ensure that the
      requests are evenly distributed across the drives.  Inevitably
      the load on one drive will be higher than on another.</p><a id="idp45010552" class="indexterm"></a><a id="idp45013624" class="indexterm"></a><p>The evenness of the load on the disks is strongly dependent
      on the way the data is shared across the drives.  In the
      following discussion, it is convenient to think of the disk
      storage as a large number of data sectors which are addressable
      by number, rather like the pages in a book.  The most obvious
      method is to divide the virtual disk into groups of consecutive
      sectors the size of the individual physical disks and store them
      in this manner, rather like taking a large book and tearing it
      into smaller sections.  This method is called
      <span class="emphasis"><em>concatenation</em></span> and has the advantage that
      the disks are not required to have any specific size
      relationships.  It works well when the access to the virtual
      disk is spread evenly about its address space.  When access is
      concentrated on a smaller area, the improvement is less marked.
      <a class="xref" href="#vinum-concat" title="Figure 1. Concatenated Organization">Figure 1, &#8220;Concatenated Organization&#8221;</a> illustrates the sequence in
      which storage units are allocated in a concatenated
      organization.</p><p>
      </p><div class="figure"><a id="vinum-concat"></a><div class="figure-title">Figure 1. Concatenated Organization</div><div class="figure-contents"><div class="mediaobject"><img src="vinum-concat.png" alt="Concatenated Organization" /></div></div></div><p><br class="figure-break" /></p><a id="idp45038328" class="indexterm"></a><a id="idp45044344" class="indexterm"></a><a id="idp45046904" class="indexterm"></a><p>An alternative mapping is to divide the address space into
      smaller, equal-sized components and store them sequentially on
      different devices.  For example, the first 256 sectors may be
      stored on the first disk, the next 256 sectors on the next disk
      and so on.  After filling the last disk, the process repeats
      until the disks are full.  This mapping is called
      <span class="emphasis"><em>striping</em></span> or
      <acronym class="acronym">RAID-0</acronym>.</p><p><acronym class="acronym">RAID</acronym> offers various forms of fault
      tolerance, though <acronym class="acronym">RAID-0</acronym> is somewhat
      misleading as it provides no redundancy.  Striping requires
      somewhat more effort to locate the data, and it can cause
      additional I/O load where a transfer is spread over multiple
      disks, but it can also provide a more constant load across the
      disks.  <a class="xref" href="#vinum-striped" title="Figure 2. Striped Organization">Figure 2, &#8220;Striped Organization&#8221;</a> illustrates the
      sequence in which storage units are allocated in a striped
      organization.</p><p>
      </p><div class="figure"><a id="vinum-striped"></a><div class="figure-title">Figure 2. Striped Organization</div><div class="figure-contents"><div class="mediaobject"><img src="vinum-striped.png" alt="Striped Organization" /></div></div></div><p><br class="figure-break" /></p></div><div class="sect1"><div xmlns="" class="titlepage"><div><div><h2 xmlns="http://www.w3.org/1999/xhtml" class="title" style="clear: both"><a id="vinum-data-integrity"></a>3. Data Integrity</h2></div></div></div><p>The final problem with disks is that they are unreliable.
      Although reliability has increased tremendously over the last
      few years, disk drives are still the most likely core component
      of a server to fail.  When they do, the results can be
      catastrophic and replacing a failed disk drive and restoring
      data can result in server downtime.</p><a id="idp45063544" class="indexterm"></a><a id="idp45065336" class="indexterm"></a><a id="idp45067896" class="indexterm"></a><p>One approach to this problem is
      <span class="emphasis"><em>mirroring</em></span>, or
      <acronym class="acronym">RAID-1</acronym>, which keeps two copies of the
      data on different physical hardware.  Any write to the volume
      writes to both disks; a read can be satisfied from either, so if
      one drive fails, the data is still available on the other
      drive.</p><p>Mirroring has two problems:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>It requires twice as much disk storage as a
	  non-redundant solution.</p></li><li class="listitem"><p>Writes must be performed to both drives, so they take up
	  twice the bandwidth of a non-mirrored volume.  Reads do not
	  suffer from a performance penalty and can even be
	  faster.</p></li></ul></div><a id="idp45095288" class="indexterm"></a><p>An alternative solution is <span class="emphasis"><em>parity</em></span>,
      implemented in <acronym class="acronym">RAID</acronym> levels 2, 3, 4 and 5.
      Of these, <acronym class="acronym">RAID-5</acronym> is the most interesting.  As
      implemented in <code class="filename">vinum</code>, it is a variant
      on a striped organization which dedicates one block of each
      stripe to parity one of the other blocks.  As implemented by
      <code class="filename">vinum</code>, a
      <acronym class="acronym">RAID-5</acronym> plex is similar to a striped plex,
      except that it implements <acronym class="acronym">RAID-5</acronym> by
      including a parity block in each stripe.  As required by
      <acronym class="acronym">RAID-5</acronym>, the location of this parity block
      changes from one stripe to the next.  The numbers in the data
      blocks indicate the relative block numbers.</p><p>
      </p><div class="figure"><a id="vinum-raid5-org"></a><div class="figure-title">Figure 3. <acronym class="acronym">RAID</acronym>-5 Organization</div><div class="figure-contents"><div class="mediaobject"><img src="vinum-raid5-org.png" alt="RAID-5 Organization" /></div></div></div><p><br class="figure-break" /></p><p>Compared to mirroring, <acronym class="acronym">RAID-5</acronym> has the
      advantage of requiring significantly less storage space.  Read
      access is similar to that of striped organizations, but write
      access is significantly slower, approximately 25% of the read
      performance.  If one drive fails, the array can continue to
      operate in degraded mode where a read from one of the remaining
      accessible drives continues normally, but a read from the
      failed drive is recalculated from the corresponding block from
      all the remaining drives.</p></div><div class="sect1"><div xmlns="" class="titlepage"><div><div><h2 xmlns="http://www.w3.org/1999/xhtml" class="title" style="clear: both"><a id="vinum-objects"></a>4. <code class="filename">vinum</code> Objects</h2></div></div></div><p>In order to address these problems,
      <code class="filename">vinum</code> implements a four-level hierarchy
      of objects:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>The most visible object is the virtual disk, called a
	  <span class="emphasis"><em>volume</em></span>.  Volumes have essentially the
	  same properties as a <span class="trademark">UNIX</span>® disk drive, though there are
	  some minor differences.  For one, they have no size
	  limitations.</p></li><li class="listitem"><p>Volumes are composed of <span class="emphasis"><em>plexes</em></span>,
	  each of which represent the total address space of a
	  volume.  This level in the hierarchy provides redundancy.
	  Think of plexes as individual disks in a mirrored array,
	  each containing the same data.</p></li><li class="listitem"><p>Since <code class="filename">vinum</code> exists within the
	  <span class="trademark">UNIX</span>® disk storage framework, it would be possible to use
	  <span class="trademark">UNIX</span>® partitions as the building block for multi-disk
	  plexes.  In fact, this turns out to be too inflexible as
	  <span class="trademark">UNIX</span>® disks can have only a limited number of partitions.
	  Instead, <code class="filename">vinum</code> subdivides a single
	  <span class="trademark">UNIX</span>® partition, the <span class="emphasis"><em>drive</em></span>, into
	  contiguous areas called <span class="emphasis"><em>subdisks</em></span>, which
	  are used as building blocks for plexes.</p></li><li class="listitem"><p>Subdisks reside on <code class="filename">vinum</code>
	  <span class="emphasis"><em>drives</em></span>, currently <span class="trademark">UNIX</span>® partitions.
	  <code class="filename">vinum</code> drives can contain any
	  number of subdisks.  With the exception of a small area at
	  the beginning of the drive, which is used for storing
	  configuration and state information, the entire drive is
	  available for data storage.</p></li></ul></div><p>The following sections describe the way these objects
      provide the functionality required of
      <code class="filename">vinum</code>.</p><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45151096"></a>4.1. Volume Size Considerations</h3></div></div></div><p>Plexes can include multiple subdisks spread over all
	drives in the <code class="filename">vinum</code> configuration.
	As a result, the size of an individual drive does not limit
	the size of a plex or a volume.</p></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45169016"></a>4.2. Redundant Data Storage</h3></div></div></div><p><code class="filename">vinum</code> implements mirroring by
	attaching multiple plexes to a volume.  Each plex is a
	representation of the data in a volume.  A volume may contain
	between one and eight plexes.</p><p>Although a plex represents the complete data of a volume,
	it is possible for parts of the representation to be
	physically missing, either by design (by not defining a
	subdisk for parts of the plex) or by accident (as a result of
	the failure of a drive).  As long as at least one plex can
	provide the data for the complete address range of the volume,
	the volume is fully functional.</p></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45173240"></a>4.3. Which Plex Organization?</h3></div></div></div><p><code class="filename">vinum</code> implements both
	concatenation and striping at the plex level:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>A <span class="emphasis"><em>concatenated plex</em></span> uses the
	    address space of each subdisk in turn.  Concatenated
	    plexes are the most flexible as they can contain any
	    number of subdisks, and the subdisks may be of different
	    length.  The plex may be extended by adding additional
	    subdisks.  They require less <acronym class="acronym">CPU</acronym>
	    time than striped plexes, though the difference in
	    <acronym class="acronym">CPU</acronym> overhead is not measurable.  On
	    the other hand, they are most susceptible to hot spots,
	    where one disk is very active and others are idle.</p></li><li class="listitem"><p>A <span class="emphasis"><em>striped plex</em></span> stripes the data
	    across each subdisk.  The subdisks must all be the same
	    size and there must be at least two subdisks in order to
	    distinguish it from a concatenated plex.  The greatest
	    advantage of striped plexes is that they reduce hot spots.
	    By choosing an optimum sized stripe, about 256 kB,
	    the load can be evened out on the component drives.
	    Extending a plex by adding new subdisks is so complicated
	    that <code class="filename">vinum</code> does not implement
	    it.</p></li></ul></div><p><a class="xref" href="#vinum-comparison" title="Table 1. vinum Plex Organizations">Table 1, &#8220;<code class="filename">vinum</code> Plex
	  Organizations&#8221;</a> summarizes the
	advantages and disadvantages of each plex organization.</p><div class="table"><a id="vinum-comparison"></a><div class="table-title">Table 1. <code class="filename">vinum</code> Plex
	  Organizations</div><div class="table-contents"><table class="table" summary="vinum Plex&#10;&#9;  Organizations" border="0"><colgroup><col /><col /><col /><col /><col /></colgroup><thead><tr><th>Plex type</th><th>Minimum subdisks</th><th>Can add subdisks</th><th>Must be equal size</th><th>Application</th></tr></thead><tbody><tr><td>concatenated</td><td>1</td><td>yes</td><td>no</td><td>Large data storage with maximum placement
		flexibility and moderate performance</td></tr><tr><td>striped</td><td>2</td><td>no</td><td>yes</td><td>High performance in combination with highly
		concurrent access</td></tr></tbody></table></div></div><br class="table-break" /></div></div><div class="sect1"><div xmlns="" class="titlepage"><div><div><h2 xmlns="http://www.w3.org/1999/xhtml" class="title" style="clear: both"><a id="vinum-examples"></a>5. Some Examples</h2></div></div></div><p><code class="filename">vinum</code> maintains a
      <span class="emphasis"><em>configuration database</em></span> which describes the
      objects known to an individual system.  Initially, the user
      creates the configuration database from one or more
      configuration files using <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=gvinum&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">gvinum</span>(8)</span></a>.
      <code class="filename">vinum</code> stores a copy of its
      configuration database on each disk
      <span class="emphasis"><em>device</em></span> under its control.  This database is
      updated on each state change, so that a restart accurately
      restores the state of each
      <code class="filename">vinum</code> object.</p><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45243000"></a>5.1. The Configuration File</h3></div></div></div><p>The configuration file describes individual
	<code class="filename">vinum</code> objects.  The definition of a
	simple volume might be:</p><pre class="programlisting">    drive a device /dev/da3h
    volume myvol
      plex org concat
        sd length 512m drive a</pre><p>This file describes four <code class="filename">vinum</code>
	objects:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>The <span class="emphasis"><em>drive</em></span> line describes a disk
	    partition (<span class="emphasis"><em>drive</em></span>) and its location
	    relative to the underlying hardware.  It is given the
	    symbolic name <span class="emphasis"><em>a</em></span>.  This separation of
	    symbolic names from device names allows disks to be moved
	    from one location to another without confusion.</p></li><li class="listitem"><p>The <span class="emphasis"><em>volume</em></span> line describes a
	    volume.  The only required attribute is the name, in this
	    case <span class="emphasis"><em>myvol</em></span>.</p></li><li class="listitem"><p>The <span class="emphasis"><em>plex</em></span> line defines a plex.
	    The only required parameter is the organization, in this
	    case <span class="emphasis"><em>concat</em></span>.  No name is necessary as
	    the system automatically generates a name from the volume
	    name by adding the suffix
	    <span class="emphasis"><em>.p</em></span><span class="emphasis"><em>x</em></span>, where
	    <span class="emphasis"><em>x</em></span> is the number of the plex in the
	    volume.  Thus this plex will be called
	    <span class="emphasis"><em>myvol.p0</em></span>.</p></li><li class="listitem"><p>The <span class="emphasis"><em>sd</em></span> line describes a subdisk.
	    The minimum specifications are the name of a drive on
	    which to store it, and the length of the subdisk.  No name
	    is necessary as the system automatically assigns names
	    derived from the plex name by adding the suffix
	    <span class="emphasis"><em>.s</em></span><span class="emphasis"><em>x</em></span>, where
	    <span class="emphasis"><em>x</em></span> is the number of the subdisk in
	    the plex.  Thus <code class="filename">vinum</code> gives this
	    subdisk the name <span class="emphasis"><em>myvol.p0.s0</em></span>.</p></li></ul></div><p>After processing this file, <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=gvinum&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">gvinum</span>(8)</span></a> produces the
	following output:</p><pre class="programlisting" width="97">
<code class="prompt">#</code> gvinum -&gt; <strong class="userinput"><code>create config1</code></strong>
Configuration summary
Drives:         1 (4 configured)
Volumes:        1 (4 configured)
Plexes:         1 (8 configured)
Subdisks:       1 (16 configured)

  D a                     State: up       Device /dev/da3h      Avail: 2061/2573 MB (80%)

  V myvol                 State: up       Plexes:       1 Size:      512 MB

  P myvol.p0            C State: up       Subdisks:     1 Size:      512 MB

  S myvol.p0.s0           State: up       PO:        0  B Size:      512 MB</pre><p>This output shows the brief listing format of
	<a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=gvinum&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">gvinum</span>(8)</span></a>.  It is represented graphically in <a class="xref" href="#vinum-simple-vol" title="Figure 4. A Simple vinum Volume">Figure 4, &#8220;A Simple <code class="filename">vinum</code>
	    Volume&#8221;</a>.</p><p>
	</p><div class="figure"><a id="vinum-simple-vol"></a><div class="figure-title">Figure 4. A Simple <code class="filename">vinum</code>
	    Volume</div><div class="figure-contents"><div class="mediaobject"><img src="vinum-simple-vol.png" alt="A Simple vinum Volume" /></div></div></div><p><br class="figure-break" /></p><p>This figure, and the ones which follow, represent a
	volume, which contains the plexes, which in turn contains the
	subdisks.  In this example, the volume contains one plex, and
	the plex contains one subdisk.</p><p>This particular volume has no specific advantage over a
	conventional disk partition.  It contains a single plex, so it
	is not redundant.  The plex contains a single subdisk, so
	there is no difference in storage allocation from a
	conventional disk partition.  The following sections
	illustrate various more interesting configuration
	methods.</p></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45315320"></a>5.2. Increased Resilience: Mirroring</h3></div></div></div><p>The resilience of a volume can be increased by mirroring.
	When laying out a mirrored volume, it is important to ensure
	that the subdisks of each plex are on different drives, so
	that a drive failure will not take down both plexes.  The
	following configuration mirrors a volume:</p><pre class="programlisting">	drive b device /dev/da4h
	volume mirror
      plex org concat
        sd length 512m drive a
	  plex org concat
	    sd length 512m drive b</pre><p>In this example, it was not necessary to specify a
	definition of drive <span class="emphasis"><em>a</em></span> again, since
	<code class="filename">vinum</code> keeps track of all objects in
	its configuration database.  After processing this definition,
	the configuration looks like:</p><pre class="programlisting" width="97">
	Drives:         2 (4 configured)
	Volumes:        2 (4 configured)
	Plexes:         3 (8 configured)
	Subdisks:       3 (16 configured)

	D a                     State: up       Device /dev/da3h       Avail: 1549/2573 MB (60%)
	D b                     State: up       Device /dev/da4h       Avail: 2061/2573 MB (80%)

    V myvol                 State: up       Plexes:       1 Size:        512 MB
    V mirror                State: up       Plexes:       2 Size:        512 MB

    P myvol.p0            C State: up       Subdisks:     1 Size:        512 MB
    P mirror.p0           C State: up       Subdisks:     1 Size:        512 MB
    P mirror.p1           C State: initializing     Subdisks:     1 Size:        512 MB

    S myvol.p0.s0           State: up       PO:        0  B Size:        512 MB
	S mirror.p0.s0          State: up       PO:        0  B Size:        512 MB
	S mirror.p1.s0          State: empty    PO:        0  B Size:        512 MB</pre><p><a class="xref" href="#vinum-mirrored-vol" title="Figure 5. A Mirrored vinum Volume">Figure 5, &#8220;A Mirrored <code class="filename">vinum</code>
	    Volume&#8221;</a> shows the
	structure graphically.</p><p>
	</p><div class="figure"><a id="vinum-mirrored-vol"></a><div class="figure-title">Figure 5. A Mirrored <code class="filename">vinum</code>
	    Volume</div><div class="figure-contents"><div class="mediaobject"><img src="vinum-mirrored-vol.png" alt="A Mirrored vinum Volume" /></div></div></div><p><br class="figure-break" /></p><p>In this example, each plex contains the full 512 MB
	of address space.  As in the previous example, each plex
	contains only a single subdisk.</p></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45331576"></a>5.3. Optimizing Performance</h3></div></div></div><p>The mirrored volume in the previous example is more
	resistant to failure than an unmirrored volume, but its
	performance is less as each write to the volume requires a
	write to both drives, using up a greater proportion of the
	total disk bandwidth.  Performance considerations demand a
	different approach:  instead of mirroring, the data is striped
	across as many disk drives as possible.  The following
	configuration shows a volume with a plex striped across four
	disk drives:</p><pre class="programlisting">        drive c device /dev/da5h
	drive d device /dev/da6h
	volume stripe
	plex org striped 512k
	  sd length 128m drive a
	  sd length 128m drive b
	  sd length 128m drive c
	  sd length 128m drive d</pre><p>As before, it is not necessary to define the drives which
	are already known to <code class="filename">vinum</code>.  After
	processing this definition, the configuration looks
	like:</p><pre class="programlisting" width="92">
	Drives:         4 (4 configured)
	Volumes:        3 (4 configured)
	Plexes:         4 (8 configured)
	Subdisks:       7 (16 configured)

    D a                     State: up       Device /dev/da3h        Avail: 1421/2573 MB (55%)
    D b                     State: up       Device /dev/da4h        Avail: 1933/2573 MB (75%)
    D c                     State: up       Device /dev/da5h        Avail: 2445/2573 MB (95%)
    D d                     State: up       Device /dev/da6h        Avail: 2445/2573 MB (95%)

    V myvol                 State: up       Plexes:       1 Size:        512 MB
    V mirror                State: up       Plexes:       2 Size:        512 MB
    V striped               State: up       Plexes:       1 Size:        512 MB

    P myvol.p0            C State: up       Subdisks:     1 Size:        512 MB
    P mirror.p0           C State: up       Subdisks:     1 Size:        512 MB
    P mirror.p1           C State: initializing     Subdisks:     1 Size:        512 MB
    P striped.p1            State: up       Subdisks:     1 Size:        512 MB

    S myvol.p0.s0           State: up       PO:        0  B Size:        512 MB
    S mirror.p0.s0          State: up       PO:        0  B Size:        512 MB
    S mirror.p1.s0          State: empty    PO:        0  B Size:        512 MB
    S striped.p0.s0         State: up       PO:        0  B Size:        128 MB
    S striped.p0.s1         State: up       PO:      512 kB Size:        128 MB
    S striped.p0.s2         State: up       PO:     1024 kB Size:        128 MB
    S striped.p0.s3         State: up       PO:     1536 kB Size:        128 MB</pre><p>
	</p><div class="figure"><a id="vinum-striped-vol"></a><div class="figure-title">Figure 6. A Striped <code class="filename">vinum</code>
	    Volume</div><div class="figure-contents"><div class="mediaobject"><img src="vinum-striped-vol.png" alt="A Striped vinum Volume" /></div></div></div><p><br class="figure-break" /></p><p>This volume is represented in <a class="xref" href="#vinum-striped-vol" title="Figure 6. A Striped vinum Volume">Figure 6, &#8220;A Striped <code class="filename">vinum</code>
	    Volume&#8221;</a>.  The darkness of the
	stripes indicates the position within the plex address space,
	where the lightest stripes come first and the darkest
	last.</p></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45347448"></a>5.4. Resilience and Performance</h3></div></div></div><p><a id="vinum-resilience"></a>With sufficient hardware,
	it is possible to build volumes which show both increased
	resilience and increased performance compared to standard
	<span class="trademark">UNIX</span>® partitions.  A typical configuration file might
	be:</p><pre class="programlisting">	volume raid10
      plex org striped 512k
        sd length 102480k drive a
        sd length 102480k drive b
        sd length 102480k drive c
        sd length 102480k drive d
        sd length 102480k drive e
      plex org striped 512k
        sd length 102480k drive c
        sd length 102480k drive d
        sd length 102480k drive e
        sd length 102480k drive a
        sd length 102480k drive b</pre><p>The subdisks of the second plex are offset by two drives
	from those of the first plex.  This helps to ensure that
	writes do not go to the same subdisks even if a transfer goes
	over two drives.</p><p><a class="xref" href="#vinum-raid10-vol" title="Figure 7. A Mirrored, Striped vinum Volume">Figure 7, &#8220;A Mirrored, Striped <code class="filename">vinum</code>
	    Volume&#8221;</a> represents the
	structure of this volume.</p><p>
	</p><div class="figure"><a id="vinum-raid10-vol"></a><div class="figure-title">Figure 7. A Mirrored, Striped <code class="filename">vinum</code>
	    Volume</div><div class="figure-contents"><div class="mediaobject"><img src="vinum-raid10-vol.png" alt="A Mirrored, Striped vinum Volume" /></div></div></div><p><br class="figure-break" /></p></div></div><div class="sect1"><div xmlns="" class="titlepage"><div><div><h2 xmlns="http://www.w3.org/1999/xhtml" class="title" style="clear: both"><a id="vinum-object-naming"></a>6. Object Naming</h2></div></div></div><p><code class="filename">vinum</code> assigns default names to
      plexes and subdisks, although they may be overridden.
      Overriding the default names is not recommended as it does not
      bring a significant advantage and it can cause
      confusion.</p><p>Names may contain any non-blank character, but it is
      recommended to restrict them to letters, digits and the
      underscore characters.  The names of volumes, plexes, and
      subdisks may be up to 64 characters long, and the names of
      drives may be up to 32 characters long.</p><p><code class="filename">vinum</code> objects are assigned device
      nodes in the hierarchy <code class="filename">/dev/gvinum</code>.  The configuration
      shown above would cause <code class="filename">vinum</code> to create
      the following device nodes:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>Device entries for each volume.  These are the main
	  devices used by <code class="filename">vinum</code>.  The
	  configuration above would include the devices
	  <code class="filename">/dev/gvinum/myvol</code>,
	  <code class="filename">/dev/gvinum/mirror</code>,
	  <code class="filename">/dev/gvinum/striped</code>,
	  <code class="filename">/dev/gvinum/raid5</code>
	  and <code class="filename">/dev/gvinum/raid10</code>.</p></li><li class="listitem"><p>All volumes get direct entries under
	  <code class="filename">/dev/gvinum/</code>.</p></li><li class="listitem"><p>The directories
	  <code class="filename">/dev/gvinum/plex</code>, and
	  <code class="filename">/dev/gvinum/sd</code>, which
	  contain device nodes for each plex and for each subdisk,
	  respectively.</p></li></ul></div><p>For example, consider the following configuration
      file:</p><pre class="programlisting">	drive drive1 device /dev/sd1h
	drive drive2 device /dev/sd2h
	drive drive3 device /dev/sd3h
	drive drive4 device /dev/sd4h
    volume s64 setupstate
      plex org striped 64k
        sd length 100m drive drive1
        sd length 100m drive drive2
        sd length 100m drive drive3
        sd length 100m drive drive4</pre><p>After processing this file, <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=gvinum&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">gvinum</span>(8)</span></a> creates the
      following structure in <code class="filename">/dev/gvinum</code>:</p><pre class="programlisting">	drwxr-xr-x  2 root  wheel       512 Apr 13
16:46 plex
	crwxr-xr--  1 root  wheel   91,   2 Apr 13 16:46 s64
	drwxr-xr-x  2 root  wheel       512 Apr 13 16:46 sd

    /dev/vinum/plex:
    total 0
    crwxr-xr--  1 root  wheel   25, 0x10000002 Apr 13 16:46 s64.p0

    /dev/vinum/sd:
    total 0
    crwxr-xr--  1 root  wheel   91, 0x20000002 Apr 13 16:46 s64.p0.s0
    crwxr-xr--  1 root  wheel   91, 0x20100002 Apr 13 16:46 s64.p0.s1
    crwxr-xr--  1 root  wheel   91, 0x20200002 Apr 13 16:46 s64.p0.s2
    crwxr-xr--  1 root  wheel   91, 0x20300002 Apr 13 16:46 s64.p0.s3</pre><p>Although it is recommended that plexes and subdisks should
      not be allocated specific names,
      <code class="filename">vinum</code> drives must be named.  This makes
      it possible to move a drive to a different location and still
      recognize it automatically.  Drive names may be up to 32
      characters long.</p><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45431288"></a>6.1. Creating File Systems</h3></div></div></div><p>Volumes appear to the system to be identical to disks,
	with one exception.  Unlike <span class="trademark">UNIX</span>® drives,
	<code class="filename">vinum</code> does not partition volumes,
	which thus do not contain a partition table.  This has
	required modification to some disk utilities, notably
	<a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=newfs&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">newfs</span>(8)</span></a>, so that it does not try to interpret the last
	letter of a <code class="filename">vinum</code> volume name as a
	partition identifier.  For example, a disk drive may have a
	name like <code class="filename">/dev/ad0a</code>
	or <code class="filename">/dev/da2h</code>.  These
	names represent the first partition
	(<code class="filename">a</code>) on the first (0) IDE disk
	(<code class="filename">ad</code>) and the eighth partition
	(<code class="filename">h</code>) on the third (2) SCSI disk
	(<code class="filename">da</code>) respectively.  By contrast, a
	<code class="filename">vinum</code> volume might be called
	<code class="filename">/dev/gvinum/concat</code>,
	which has no relationship with a partition name.</p><p>In order to create a file system on this volume, use
	<a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=newfs&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">newfs</span>(8)</span></a>:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>newfs /dev/gvinum/concat</code></strong></pre></div></div><div class="sect1"><div xmlns="" class="titlepage"><div><div><h2 xmlns="http://www.w3.org/1999/xhtml" class="title" style="clear: both"><a id="vinum-config"></a>7. Configuring <code class="filename">vinum</code></h2></div></div></div><p>The <code class="filename">GENERIC</code> kernel does not contain
      <code class="filename">vinum</code>.  It is possible to build a
      custom kernel which includes <code class="filename">vinum</code>, but
      this is not recommended.  The standard way to start
      <code class="filename">vinum</code> is as a kernel module.
      <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=kldload&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">kldload</span>(8)</span></a> is not needed because when <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=gvinum&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">gvinum</span>(8)</span></a>
      starts, it checks whether the module has been loaded, and if it
      is not, it loads it automatically.</p><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45474808"></a>7.1. Startup</h3></div></div></div><p><code class="filename">vinum</code> stores configuration
	information on the disk slices in essentially the same form as
	in the configuration files.  When reading from the
	configuration database, <code class="filename">vinum</code>
	recognizes a number of keywords which are not allowed in the
	configuration files.  For example, a disk configuration might
	contain the following text:</p><pre class="programlisting" width="119">volume myvol state up
volume bigraid state down
plex name myvol.p0 state up org concat vol myvol
plex name myvol.p1 state up org concat vol myvol
plex name myvol.p2 state init org striped 512b vol myvol
plex name bigraid.p0 state initializing org raid5 512b vol bigraid
sd name myvol.p0.s0 drive a plex myvol.p0 state up len 1048576b driveoffset 265b plexoffset 0b
sd name myvol.p0.s1 drive b plex myvol.p0 state up len 1048576b driveoffset 265b plexoffset 1048576b
sd name myvol.p1.s0 drive c plex myvol.p1 state up len 1048576b driveoffset 265b plexoffset 0b
sd name myvol.p1.s1 drive d plex myvol.p1 state up len 1048576b driveoffset 265b plexoffset 1048576b
sd name myvol.p2.s0 drive a plex myvol.p2 state init len 524288b driveoffset 1048841b plexoffset 0b
sd name myvol.p2.s1 drive b plex myvol.p2 state init len 524288b driveoffset 1048841b plexoffset 524288b
sd name myvol.p2.s2 drive c plex myvol.p2 state init len 524288b driveoffset 1048841b plexoffset 1048576b
sd name myvol.p2.s3 drive d plex myvol.p2 state init len 524288b driveoffset 1048841b plexoffset 1572864b
sd name bigraid.p0.s0 drive a plex bigraid.p0 state initializing len 4194304b driveoff set 1573129b plexoffset 0b
sd name bigraid.p0.s1 drive b plex bigraid.p0 state initializing len 4194304b driveoff set 1573129b plexoffset 4194304b
sd name bigraid.p0.s2 drive c plex bigraid.p0 state initializing len 4194304b driveoff set 1573129b plexoffset 8388608b
sd name bigraid.p0.s3 drive d plex bigraid.p0 state initializing len 4194304b driveoff set 1573129b plexoffset 12582912b
sd name bigraid.p0.s4 drive e plex bigraid.p0 state initializing len 4194304b driveoff set 1573129b plexoffset 16777216b</pre><p>The obvious differences here are the presence of
	  explicit location information and naming, both of which are
	  allowed but discouraged, and the information on the states.
	  <code class="filename">vinum</code> does not store information
	  about drives in the configuration information.  It finds the
	  drives by scanning the configured disk drives for partitions
	  with a <code class="filename">vinum</code> label.  This enables
	  <code class="filename">vinum</code> to identify drives correctly
	  even if they have been assigned different <span class="trademark">UNIX</span>® drive
	  IDs.</p><div class="sect3"><div xmlns="" class="titlepage"><div><div><h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="vinum-rc-startup"></a>7.1.1. Automatic Startup</h4></div></div></div><p><span class="emphasis"><em>Gvinum</em></span> always features an
	    automatic startup once the kernel module is loaded, via
	    <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=loader.conf&amp;sektion=5&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">loader.conf</span>(5)</span></a>.  To load the
	    <span class="emphasis"><em>Gvinum</em></span> module at boot time, add
	    <code class="literal">geom_vinum_load="YES"</code> to
	    <code class="filename">/boot/loader.conf</code>.</p><p>When <code class="filename">vinum</code> is started with
	    <code class="command">gvinum start</code>,
	    <code class="filename">vinum</code> reads the configuration
	    database from one of the <code class="filename">vinum</code>
	    drives.  Under normal circumstances, each drive contains
	    an identical copy of the configuration database, so it
	    does not matter which drive is read.  After a crash,
	    however, <code class="filename">vinum</code> must determine
	    which drive was updated most recently and read the
	    configuration from this drive.  It then updates the
	    configuration, if necessary, from progressively older
	    drives.</p></div></div></div><div class="sect1"><div xmlns="" class="titlepage"><div><div><h2 xmlns="http://www.w3.org/1999/xhtml" class="title" style="clear: both"><a id="vinum-root"></a>8. Using <code class="filename">vinum</code> for the Root
	File System</h2></div></div></div><p>For a machine that has fully-mirrored file systems using
	<code class="filename">vinum</code>, it is desirable to also
	mirror the root file system.  Setting up such a configuration
	is less trivial than mirroring an arbitrary file system
	because:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>The root file system must be available very early
	    during the boot process, so the
	    <code class="filename">vinum</code> infrastructure must
	    already be available at this time.</p></li><li class="listitem"><p>The volume containing the root file system also
	    contains the system bootstrap and the kernel.  These must
	    be read using the host system's native utilities, such as
	    the BIOS, which often cannot be taught about the details
	    of <code class="filename">vinum</code>.</p></li></ul></div><p>In the following sections, the term <span class="quote">&#8220;<span class="quote">root
	  volume</span>&#8221;</span> is generally used to describe the
	<code class="filename">vinum</code> volume that contains the root
	file system.</p><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45524728"></a>8.1. Starting up <code class="filename">vinum</code> Early
	  Enough for the Root File System</h3></div></div></div><p><code class="filename">vinum</code> must be available early
	  in the system boot as <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=loader&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">loader</span>(8)</span></a> must be able to load
	  the vinum kernel module before starting the kernel.  This
	  can be accomplished by putting this line in
	  <code class="filename">/boot/loader.conf</code>:</p><pre class="programlisting">geom_vinum_load="YES"</pre></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45531640"></a>8.2. Making a <code class="filename">vinum</code>-based Root
	Volume Accessible to the Bootstrap</h3></div></div></div><p>The current FreeBSD bootstrap is only 7.5 KB of code and
	does not understand the internal
	<code class="filename">vinum</code> structures.  This means that it
	cannot parse the <code class="filename">vinum</code> configuration
	data or figure out the elements of a boot volume.  Thus, some
	workarounds are necessary to provide the bootstrap code with
	the illusion of a standard <code class="literal">a</code> partition
	that contains the root file system.</p><p>For this to be possible, the following requirements must
	be met for the root volume:</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p>The root volume must not be a stripe or
	    <acronym class="acronym">RAID</acronym>-5.</p></li><li class="listitem"><p>The root volume must not contain more than one
	    concatenated subdisk per plex.</p></li></ul></div><p>Note that it is desirable and possible to use multiple
	plexes, each containing one replica of the root file system.
	The bootstrap process will only use one replica for finding
	the bootstrap and all boot files, until the kernel mounts the
	root file system.  Each single subdisk within these plexes
	needs its own <code class="literal">a</code> partition illusion, for
	the respective device to be bootable.  It is not strictly
	needed that each of these faked <code class="literal">a</code>
	partitions is located at the same offset within its device,
	compared with other devices containing plexes of the root
	volume.  However, it is probably a good idea to create the
	<code class="filename">vinum</code> volumes that way so the
	resulting mirrored devices are symmetric, to avoid
	confusion.</p><p>In order to set up these <code class="literal">a</code>
	partitions for each device containing part of the root
	volume, the following is required:</p><div class="procedure"><ol class="procedure" type="1"><li class="step"><p>The location, offset from the beginning of the device,
	    and size of this device's subdisk that is part of the root
	    volume needs to be examined, using the command:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>gvinum l -rv root</code></strong></pre><p><code class="filename">vinum</code> offsets and sizes are
	    measured in bytes.  They must be divided by 512 in order
	    to obtain the block numbers that are to be used by
	    <code class="command">bsdlabel</code>.</p></li><li class="step"><p>Run this command for each device that participates in
	    the root volume:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>bsdlabel -e <em class="replaceable"><code>devname</code></em></code></strong></pre><p><em class="replaceable"><code>devname</code></em> must be either the
	    name of the disk, like <code class="filename">da0</code> for
	    disks without a slice table, or the name of the
	    slice, like <code class="filename">ad0s1</code>.</p><p>If there is already an <code class="literal">a</code>
	    partition on the device from a
	    pre-<code class="filename">vinum</code> root file system, it
	    should be renamed to something else so that it remains
	    accessible (just in case), but will no longer be used by
	    default to bootstrap the system.  A currently mounted root
	    file system cannot be renamed, so this must be executed
	    either when being booted from a <span class="quote">&#8220;<span class="quote">Fixit</span>&#8221;</span>
	    media, or in a two-step process where, in a mirror, the
	    disk that is not been currently booted is manipulated
	    first.</p><p>The offset of the <code class="filename">vinum</code>
	    partition on this device (if any) must be added to the
	    offset of the respective root volume subdisk on this
	    device.  The resulting value will become the
	    <code class="literal">offset</code> value for the new
	    <code class="literal">a</code> partition.  The
	    <code class="literal">size</code> value for this partition can be
	    taken verbatim from the calculation above.  The
	    <code class="literal">fstype</code> should be
	    <code class="literal">4.2BSD</code>.  The
	    <code class="literal">fsize</code>, <code class="literal">bsize</code>,
	    and <code class="literal">cpg</code> values should be chosen
	    to match the actual file system, though they are fairly
	    unimportant within this context.</p><p>That way, a new <code class="literal">a</code> partition will
	    be established that overlaps the
	    <code class="filename">vinum</code> partition on this device.
	    <code class="command">bsdlabel</code> will only allow for this
	    overlap if the <code class="filename">vinum</code> partition
	    has properly been marked using the
	    <code class="literal">vinum</code> fstype.</p></li><li class="step"><p>A faked <code class="literal">a</code> partition now exists
	    on each device that has one replica of the root volume.
	    It is highly recommendable to verify the result using a
	    command like:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>fsck -n /dev/<em class="replaceable"><code>devname</code></em>a</code></strong></pre></li></ol></div><p>It should be remembered that all files containing control
	information must be relative to the root file system in the
	<code class="filename">vinum</code> volume which, when setting up
	a new <code class="filename">vinum</code> root volume, might not
	match the root file system that is currently active.  So in
	particular, <code class="filename">/etc/fstab</code> and
	<code class="filename">/boot/loader.conf</code> need to be taken care
	of.</p><p>At next reboot, the bootstrap should figure out the
	appropriate control information from the new
	<code class="filename">vinum</code>-based root file system, and act
	accordingly.  At the end of the kernel initialization process,
	after all devices have been announced, the prominent notice
	that shows the success of this setup is a message like:</p><pre class="screen">Mounting root from ufs:/dev/gvinum/root</pre></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45621752"></a>8.3. Example of a <code class="filename">vinum</code>-based Root
	Setup</h3></div></div></div><p>After the <code class="filename">vinum</code> root volume has
	been set up, the output of <code class="command">gvinum l -rv
	  root</code> could look like:</p><pre class="screen">...
Subdisk root.p0.s0:
		Size:        125829120 bytes (120 MB)
		State: up
		Plex root.p0 at offset 0 (0  B)
		Drive disk0 (/dev/da0h) at offset 135680 (132 kB)

Subdisk root.p1.s0:
		Size:        125829120 bytes (120 MB)
		State: up
		Plex root.p1 at offset 0 (0  B)
		Drive disk1 (/dev/da1h) at offset 135680 (132 kB)</pre><p>The values to note are <code class="literal">135680</code> for the
	offset, relative to partition
	<code class="filename">/dev/da0h</code>.  This
	translates to 265 512-byte disk blocks in
	<code class="command">bsdlabel</code>'s terms.  Likewise, the size of
	this root volume is 245760 512-byte blocks.  <code class="filename">/dev/da1h</code>, containing the
	second replica of this root volume, has a symmetric
	setup.</p><p>The bsdlabel for these devices might look like:</p><pre class="screen">...
8 partitions:
#        size   offset    fstype   [fsize bsize bps/cpg]
  a:   245760      281    4.2BSD     2048 16384     0   # (Cyl.    0*- 15*)
  c: 71771688        0    unused        0     0         # (Cyl.    0 - 4467*)
  h: 71771672       16     vinum                        # (Cyl.    0*- 4467*)</pre><p>It can be observed that the <code class="literal">size</code>
	parameter for the faked <code class="literal">a</code> partition
	matches the value outlined above, while the
	<code class="literal">offset</code> parameter is the sum of the offset
	within the <code class="filename">vinum</code> partition
	<code class="literal">h</code>, and the offset of this partition
	within the device or slice.  This is a typical setup that is
	necessary to avoid the problem described in <a class="xref" href="#vinum-root-panic" title="8.4.3. Nothing Boots, the Bootstrap Panics">Section 8.4.3, &#8220;Nothing Boots, the Bootstrap
	  Panics&#8221;</a>.  The entire
	<code class="literal">a</code> partition is completely within the
	<code class="literal">h</code> partition containing all the
	<code class="filename">vinum</code> data for this device.</p><p>In the above example, the entire device is dedicated to
	<code class="filename">vinum</code> and there is no leftover
	pre-<code class="filename">vinum</code> root partition.</p></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45669752"></a>8.4. Troubleshooting</h3></div></div></div><p>The following list contains a few known pitfalls and
	solutions.</p><div class="sect3"><div xmlns="" class="titlepage"><div><div><h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45671544"></a>8.4.1. System Bootstrap Loads, but System Does Not
	  Boot</h4></div></div></div><p>If for any reason the system does not continue to boot,
	  the bootstrap can be interrupted by pressing
	  <span class="keycap"><strong>space</strong></span> at the 10-seconds warning.  The
	  loader variable <code class="literal">vinum.autostart</code> can be
	  examined by typing <code class="command">show</code> and manipulated
	  using <code class="command">set</code> or
	  <code class="command">unset</code>.</p><p>If the <code class="filename">vinum</code> kernel module was
	  not yet in the list of modules to load automatically, type
	  <code class="command">load geom_vinum</code>.</p><p>When ready, the boot process can be continued by typing
	  <code class="command">boot -as</code> which
	  <code class="option">-as</code> requests the kernel to ask for the
	  root file system to mount (<code class="option">-a</code>) and make the
	  boot process stop in single-user mode (<code class="option">-s</code>),
	  where the root file system is mounted read-only.  That way,
	  even if only one plex of a multi-plex volume has been
	  mounted, no data inconsistency between plexes is being
	  risked.</p><p>At the prompt asking for a root file system to mount,
	  any device that contains a valid root file system can be
	  entered.  If <code class="filename">/etc/fstab</code> is set up
	  correctly, the default should be something like
	  <code class="literal">ufs:/dev/gvinum/root</code>.  A typical
	  alternate choice would be something like
	  <code class="literal">ufs:da0d</code> which could be a
	  hypothetical partition containing the
	  pre-<code class="filename">vinum</code> root file system.  Care
	  should be taken if one of the alias
	  <code class="literal">a</code> partitions is entered here, that it
	  actually references the subdisks of the
	  <code class="filename">vinum</code> root device, because in a
	  mirrored setup, this would only mount one piece of a
	  mirrored root device.  If this file system is to be mounted
	  read-write later on, it is necessary to remove the other
	  plex(es) of the <code class="filename">vinum</code> root volume
	  since these plexes would otherwise carry inconsistent
	  data.</p></div><div class="sect3"><div xmlns="" class="titlepage"><div><div><h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp45706488"></a>8.4.2. Only Primary Bootstrap Loads</h4></div></div></div><p>If <code class="filename">/boot/loader</code> fails to load, but
	  the primary bootstrap still loads (visible by a single dash
	  in the left column of the screen right after the boot
	  process starts), an attempt can be made to interrupt the
	  primary bootstrap by pressing
	  <span class="keycap"><strong>space</strong></span>.  This will make the bootstrap stop
	  in <a class="link" href="../../../../doc/en_US.ISO8859-1/books/handbook/boot.html#boot-boot1" target="_top">stage two</a>.  An attempt
	  can be made here to boot off an alternate partition, like
	  the partition containing the previous root file system that
	  has been moved away from <code class="literal">a</code>.</p></div><div class="sect3"><div xmlns="" class="titlepage"><div><div><h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="vinum-root-panic"></a>8.4.3. Nothing Boots, the Bootstrap
	  Panics</h4></div></div></div><p>This situation will happen if the bootstrap had been
	  destroyed by the <code class="filename">vinum</code>
	  installation.  Unfortunately, <code class="filename">vinum</code>
	  accidentally leaves only 4 KB at the beginning of its
	  partition free before starting to write its
	  <code class="filename">vinum</code> header information.  However,
	  the stage one and two bootstraps plus the bsdlabel require 8
	  KB.  So if a <code class="filename">vinum</code> partition was
	  started at offset 0 within a slice or disk that was meant to
	  be bootable, the <code class="filename">vinum</code> setup will
	  trash the bootstrap.</p><p>Similarly, if the above situation has been recovered,
	  by booting from a <span class="quote">&#8220;<span class="quote">Fixit</span>&#8221;</span> media, and the
	  bootstrap has been re-installed using
	  <code class="command">bsdlabel -B</code> as described in <a class="link" href="../../../../doc/en_US.ISO8859-1/books/handbook/boot.html#boot-boot1" target="_top">../../../../doc/en_US.ISO8859-1/books/handbook/boot.html#boot-boot1</a>, the bootstrap will trash the
	  <code class="filename">vinum</code> header, and
	  <code class="filename">vinum</code> will no longer find its
	  disk(s).  Though no actual <code class="filename">vinum</code>
	  configuration data or data in <code class="filename">vinum</code>
	  volumes will be trashed, and it would be possible to recover
	  all the data by entering exactly the same
	  <code class="filename">vinum</code> configuration data again, the
	  situation is hard to fix.  It is necessary to move the
	  entire <code class="filename">vinum</code> partition by at least
	  4 KB, in order to have the <code class="filename">vinum</code>
	  header and the system bootstrap no longer collide.</p></div></div></div></div></body></html>