<?xml version="1.0" encoding="iso-8859-1" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" /><title>19.3. zpool Administration</title><link rel="stylesheet" type="text/css" href="docbook.css" /><link rev="made" href="mailto:doc@FreeBSD.org" /><meta name="generator" content="DocBook XSL Stylesheets V1.79.1" /><link rel="home" href="index.html" title="FreeBSD Handbook" /><link rel="up" href="zfs.html" title="Chapter 19. The Z File System (ZFS)" /><link rel="prev" href="zfs-quickstart.html" title="19.2. Quick Start Guide" /><link rel="next" href="zfs-zfs.html" title="19.4. zfs Administration" /><link rel="copyright" href="legalnotice.html" title="Copyright" /><script xmlns="" type="text/javascript" src="/layout/js/google.js"></script></head><body><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">19.3. <code class="command">zpool</code> Administration</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="zfs-quickstart.html">Prev</a> </td><th width="60%" align="center">Chapter 19. The Z File System (<acronym class="acronym">ZFS</acronym>)</th><td width="20%" align="right"> <a accesskey="n" href="zfs-zfs.html">Next</a></td></tr></table><hr /></div><div class="sect1"><div xmlns="" class="titlepage"><div><div><h2 xmlns="http://www.w3.org/1999/xhtml" class="title" style="clear: both"><a id="zfs-zpool"></a>19.3. <code class="command">zpool</code> Administration</h2></div></div></div><p><acronym class="acronym">ZFS</acronym> administration is divided between two
      main utilities.  The <code class="command">zpool</code> utility controls
      the operation of the pool and deals with adding, removing,
      replacing, and managing disks.  The
      <a class="link" href="zfs-zfs.html" title="19.4. zfs Administration"><code class="command">zfs</code></a> utility
      deals with creating, destroying, and managing datasets,
      both <a class="link" href="zfs-term.html#zfs-term-filesystem">file systems</a> and
      <a class="link" href="zfs-term.html#zfs-term-volume">volumes</a>.</p><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-create"></a>19.3.1. Creating and Destroying Storage Pools</h3></div></div></div><p>Creating a <acronym class="acronym">ZFS</acronym> storage pool
	(<span class="emphasis"><em>zpool</em></span>) involves making a number of
	decisions that are relatively permanent because the structure
	of the pool cannot be changed after the pool has been created.
	The most important decision is what types of vdevs into which
	to group the physical disks.  See the list of
	<a class="link" href="zfs-term.html#zfs-term-vdev">vdev types</a> for details
	about the possible options.  After the pool has been created,
	most vdev types do not allow additional disks to be added to
	the vdev.  The exceptions are mirrors, which allow additional
	disks to be added to the vdev, and stripes, which can be
	upgraded to mirrors by attaching an additional disk to the
	vdev.  Although additional vdevs can be added to expand a
	pool, the layout of the pool cannot be changed after pool
	creation.  Instead, the data must be backed up and the
	pool destroyed and recreated.</p><p>Create a simple mirror pool:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool create <em class="replaceable"><code>mypool</code></em> mirror <em class="replaceable"><code>/dev/ada1</code></em> <em class="replaceable"><code>/dev/ada2</code></em></code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada1    ONLINE       0     0     0
            ada2    ONLINE       0     0     0

errors: No known data errors</pre><p>Multiple vdevs can be created at once.  Specify multiple
	groups of disks separated by the vdev type keyword,
	<code class="literal">mirror</code> in this example:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool create <em class="replaceable"><code>mypool</code></em> mirror <em class="replaceable"><code>/dev/ada1</code></em> <em class="replaceable"><code>/dev/ada2</code></em> mirror <em class="replaceable"><code>/dev/ada3</code></em> <em class="replaceable"><code>/dev/ada4</code></em></code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada1    ONLINE       0     0     0
            ada2    ONLINE       0     0     0
          mirror-1  ONLINE       0     0     0
            ada3    ONLINE       0     0     0
            ada4    ONLINE       0     0     0

errors: No known data errors</pre><p>Pools can also be constructed using partitions rather than
	whole disks.  Putting <acronym class="acronym">ZFS</acronym> in a separate
	partition allows the same disk to have other partitions for
	other purposes.  In particular, partitions with bootcode and
	file systems needed for booting can be added.  This allows
	booting from disks that are also members of a pool.  There is
	no performance penalty on FreeBSD when using a partition rather
	than a whole disk.  Using partitions also allows the
	administrator to <span class="emphasis"><em>under-provision</em></span> the
	disks, using less than the full capacity.  If a future
	replacement disk of the same nominal size as the original
	actually has a slightly smaller capacity, the smaller
	partition will still fit, and the replacement disk can still
	be used.</p><p>Create a
	<a class="link" href="zfs-term.html#zfs-term-vdev-raidz">RAID-Z2</a> pool using
	partitions:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool create <em class="replaceable"><code>mypool</code></em> raidz2 <em class="replaceable"><code>/dev/ada0p3</code></em> <em class="replaceable"><code>/dev/ada1p3</code></em> <em class="replaceable"><code>/dev/ada2p3</code></em> <em class="replaceable"><code>/dev/ada3p3</code></em> <em class="replaceable"><code>/dev/ada4p3</code></em> <em class="replaceable"><code>/dev/ada5p3</code></em></code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          raidz2-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0
            ada4p3  ONLINE       0     0     0
            ada5p3  ONLINE       0     0     0

errors: No known data errors</pre><p>A pool that is no longer needed can be destroyed so that
	the disks can be reused.  Destroying a pool involves first
	unmounting all of the datasets in that pool.  If the datasets
	are in use, the unmount operation will fail and the pool will
	not be destroyed.  The destruction of the pool can be forced
	with <code class="option">-f</code>, but this can cause undefined
	behavior in applications which had open files on those
	datasets.</p></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-attach"></a>19.3.2. Adding and Removing Devices</h3></div></div></div><p>There are two cases for adding disks to a zpool: attaching
	a disk to an existing vdev with
	<code class="command">zpool attach</code>, or adding vdevs to the pool
	with <code class="command">zpool add</code>.  Only some
	<a class="link" href="zfs-term.html#zfs-term-vdev">vdev types</a> allow disks to
	be added to the vdev after creation.</p><p>A pool created with a single disk lacks redundancy.
	Corruption can be detected but
	not repaired, because there is no other copy of the data.

	The <a class="link" href="zfs-term.html#zfs-term-copies">copies</a> property may
	be able to recover from a small failure such as a bad sector,
	but does not provide the same level of protection as mirroring
	or <acronym class="acronym">RAID-Z</acronym>.  Starting with a pool consisting
	of a single disk vdev, <code class="command">zpool attach</code> can be
	used to add an additional disk to the vdev, creating a mirror.
	<code class="command">zpool attach</code> can also be used to add
	additional disks to a mirror group, increasing redundancy and
	read performance.  If the disks being used for the pool are
	partitioned, replicate the layout of the first disk on to the
	second, <code class="command">gpart backup</code> and
	<code class="command">gpart restore</code> can be used to make this
	process easier.</p><p>Upgrade the single disk (stripe) vdev
	<em class="replaceable"><code>ada0p3</code></em> to a mirror by attaching
	<em class="replaceable"><code>ada1p3</code></em>:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          ada0p3    ONLINE       0     0     0

errors: No known data errors
<code class="prompt">#</code> <strong class="userinput"><code>zpool attach <em class="replaceable"><code>mypool</code></em> <em class="replaceable"><code>ada0p3</code></em> <em class="replaceable"><code>ada1p3</code></em></code></strong>
Make sure to wait until resilver is done before rebooting.

If you boot from pool 'mypool', you may need to update
boot code on newly attached disk 'ada1p3'.

Assuming you use GPT partitioning and 'da0' is your new boot disk
you may use the following command:

        gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 da0
<code class="prompt">#</code> <strong class="userinput"><code>gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 <em class="replaceable"><code>ada1</code></em></code></strong>
bootcode written to ada1
<code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
status: One or more devices is currently being resilvered.  The pool will
        continue to function, possibly in a degraded state.
action: Wait for the resilver to complete.
  scan: resilver in progress since Fri May 30 08:19:19 2014
        527M scanned out of 781M at 47.9M/s, 0h0m to go
        527M resilvered, 67.53% done
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0  (resilvering)

errors: No known data errors
<code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M in 0h0m with 0 errors on Fri May 30 08:15:58 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors</pre><p>When adding disks to the existing vdev is not an option,
	as for <acronym class="acronym">RAID-Z</acronym>, an alternative method is to
	add another vdev to the pool.  Additional vdevs provide higher
	performance, distributing writes across the vdevs.  Each vdev
	is responsible for providing its own redundancy.  It is
	possible, but discouraged, to mix vdev types, like
	<code class="literal">mirror</code> and <code class="literal">RAID-Z</code>.
	Adding a non-redundant vdev to a pool containing mirror or
	<acronym class="acronym">RAID-Z</acronym> vdevs risks the data on the entire
	pool.  Writes are distributed, so the failure of the
	non-redundant disk will result in the loss of a fraction of
	every block that has been written to the pool.</p><p>Data is striped across each of the vdevs.  For example,
	with two mirror vdevs, this is effectively a
	<acronym class="acronym">RAID</acronym> 10 that stripes writes across two sets
	of mirrors.  Space is allocated so that each vdev reaches 100%
	full at the same time.  There is a performance penalty if the
	vdevs have different amounts of free space, as a
	disproportionate amount of the data is written to the less
	full vdev.</p><p>When attaching additional devices to a boot pool, remember
	to update the bootcode.</p><p>Attach a second mirror group (<code class="filename">ada2p3</code>
	and <code class="filename">ada3p3</code>) to the existing
	mirror:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M in 0h0m with 0 errors on Fri May 30 08:19:35 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors
<code class="prompt">#</code> <strong class="userinput"><code>zpool add <em class="replaceable"><code>mypool</code></em> mirror <em class="replaceable"><code>ada2p3</code></em> <em class="replaceable"><code>ada3p3</code></em></code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 <em class="replaceable"><code>ada2</code></em></code></strong>
bootcode written to ada2
<code class="prompt">#</code> <strong class="userinput"><code>gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 <em class="replaceable"><code>ada3</code></em></code></strong>
bootcode written to ada3
<code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 in 0h0m with 0 errors on Fri May 30 08:29:51 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
          mirror-1  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0

errors: No known data errors</pre><p>Currently, vdevs cannot be removed from a pool, and disks
	can only be removed from a mirror if there is enough remaining
	redundancy.  If only one disk in a mirror group remains, it
	ceases to be a mirror and reverts to being a stripe, risking
	the entire pool if that remaining disk fails.</p><p>Remove a disk from a three-way mirror group:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 in 0h0m with 0 errors on Fri May 30 08:29:51 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0

errors: No known data errors
<code class="prompt">#</code> <strong class="userinput"><code>zpool detach <em class="replaceable"><code>mypool</code></em> <em class="replaceable"><code>ada2p3</code></em></code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 in 0h0m with 0 errors on Fri May 30 08:29:51 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors</pre></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-status"></a>19.3.3. Checking the Status of a Pool</h3></div></div></div><p>Pool status is important.  If a drive goes offline or a
	read, write, or checksum error is detected, the corresponding
	error count increases.  The <code class="command">status</code> output
	shows the configuration and status of each device in the pool
	and the status of the entire pool.  Actions that need to be
	taken and details about the last <a class="link" href="zfs-zpool.html#zfs-zpool-scrub" title="19.3.7. Scrubbing a Pool"><code class="command">scrub</code></a>
	are also shown.</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: scrub repaired 0 in 2h25m with 0 errors on Sat Sep 14 04:25:50 2013
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          raidz2-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0
            ada4p3  ONLINE       0     0     0
            ada5p3  ONLINE       0     0     0

errors: No known data errors</pre></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-clear"></a>19.3.4. Clearing Errors</h3></div></div></div><p>When an error is detected, the read, write, or checksum
	counts are incremented.  The error message can be cleared and
	the counts reset with <code class="command">zpool clear
	  <em class="replaceable"><code>mypool</code></em></code>.  Clearing the
	error state can be important for automated scripts that alert
	the administrator when the pool encounters an error.  Further
	errors may not be reported if the old errors are not
	cleared.</p></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-replace"></a>19.3.5. Replacing a Functioning Device</h3></div></div></div><p>There are a number of situations where it may be
	desirable to replace one disk with a different disk.  When
	replacing a working disk, the process keeps the old disk
	online during the replacement.  The pool never enters a
	<a class="link" href="zfs-term.html#zfs-term-degraded">degraded</a> state,
	reducing the risk of data loss.
	<code class="command">zpool replace</code> copies all of the data from
	the old disk to the new one.  After the operation completes,
	the old disk is disconnected from the vdev.  If the new disk
	is larger than the old disk, it may be possible to grow the
	zpool, using the new space.  See <a class="link" href="zfs-zpool.html#zfs-zpool-online" title="19.3.9. Growing a Pool">Growing a Pool</a>.</p><p>Replace a functioning device in the pool:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0

errors: No known data errors
<code class="prompt">#</code> <strong class="userinput"><code>zpool replace <em class="replaceable"><code>mypool</code></em> <em class="replaceable"><code>ada1p3</code></em> <em class="replaceable"><code>ada2p3</code></em></code></strong>
Make sure to wait until resilver is done before rebooting.

If you boot from pool 'zroot', you may need to update
boot code on newly attached disk 'ada2p3'.

Assuming you use GPT partitioning and 'da0' is your new boot disk
you may use the following command:

        gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 da0
<code class="prompt">#</code> <strong class="userinput"><code>gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i 1 <em class="replaceable"><code>ada2</code></em></code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
status: One or more devices is currently being resilvered.  The pool will
        continue to function, possibly in a degraded state.
action: Wait for the resilver to complete.
  scan: resilver in progress since Mon Jun  2 14:21:35 2014
        604M scanned out of 781M at 46.5M/s, 0h0m to go
        604M resilvered, 77.39% done
config:

        NAME             STATE     READ WRITE CKSUM
        mypool           ONLINE       0     0     0
          mirror-0       ONLINE       0     0     0
            ada0p3       ONLINE       0     0     0
            replacing-1  ONLINE       0     0     0
              ada1p3     ONLINE       0     0     0
              ada2p3     ONLINE       0     0     0  (resilvering)

errors: No known data errors
<code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M in 0h0m with 0 errors on Mon Jun  2 14:21:52 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0

errors: No known data errors</pre></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-resilver"></a>19.3.6. Dealing with Failed Devices</h3></div></div></div><p>When a disk in a pool fails, the vdev to which the disk
	belongs enters the
	<a class="link" href="zfs-term.html#zfs-term-degraded">degraded</a> state.  All
	of the data is still available, but performance may be reduced
	because missing data must be calculated from the available
	redundancy.  To restore the vdev to a fully functional state,
	the failed physical device must be replaced.
	<acronym class="acronym">ZFS</acronym> is then instructed to begin the
	<a class="link" href="zfs-term.html#zfs-term-resilver">resilver</a> operation.
	Data that was on the failed device is recalculated from
	available redundancy and written to the replacement device.
	After completion, the vdev returns to
	<a class="link" href="zfs-term.html#zfs-term-online">online</a> status.</p><p>If the vdev does not have any redundancy, or if multiple
	devices have failed and there is not enough redundancy to
	compensate, the pool enters the
	<a class="link" href="zfs-term.html#zfs-term-faulted">faulted</a> state.  If a
	sufficient number of devices cannot be reconnected to the
	pool, the pool becomes inoperative and data must be restored
	from backups.</p><p>When replacing a failed disk, the name of the failed disk
	is replaced with the <acronym class="acronym">GUID</acronym> of the device.
	A new device name parameter for
	<code class="command">zpool replace</code> is not required if the
	replacement device has the same device name.</p><p>Replace a failed disk using
	<code class="command">zpool replace</code>:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: DEGRADED
status: One or more devices could not be opened.  Sufficient replicas exist for
        the pool to continue functioning in a degraded state.
action: Attach the missing device and online it using 'zpool online'.
   see: http://illumos.org/msg/ZFS-8000-2Q
  scan: none requested
config:

        NAME                    STATE     READ WRITE CKSUM
        mypool                  DEGRADED     0     0     0
          mirror-0              DEGRADED     0     0     0
            ada0p3              ONLINE       0     0     0
            316502962686821739  UNAVAIL      0     0     0  was /dev/ada1p3

errors: No known data errors
<code class="prompt">#</code> <strong class="userinput"><code>zpool replace <em class="replaceable"><code>mypool</code></em> <em class="replaceable"><code>316502962686821739</code></em> <em class="replaceable"><code>ada2p3</code></em></code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: DEGRADED
status: One or more devices is currently being resilvered.  The pool will
        continue to function, possibly in a degraded state.
action: Wait for the resilver to complete.
  scan: resilver in progress since Mon Jun  2 14:52:21 2014
        641M scanned out of 781M at 49.3M/s, 0h0m to go
        640M resilvered, 82.04% done
config:

        NAME                        STATE     READ WRITE CKSUM
        mypool                      DEGRADED     0     0     0
          mirror-0                  DEGRADED     0     0     0
            ada0p3                  ONLINE       0     0     0
            replacing-1             UNAVAIL      0     0     0
              15732067398082357289  UNAVAIL      0     0     0  was /dev/ada1p3/old
              ada2p3                ONLINE       0     0     0  (resilvering)

errors: No known data errors
<code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: resilvered 781M in 0h0m with 0 errors on Mon Jun  2 14:52:38 2014
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0

errors: No known data errors</pre></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-scrub"></a>19.3.7. Scrubbing a Pool</h3></div></div></div><p>It is recommended that pools be
	<a class="link" href="zfs-term.html#zfs-term-scrub">scrubbed</a> regularly,
	ideally at least once every month.  The
	<code class="command">scrub</code> operation is very disk-intensive and
	will reduce performance while running.  Avoid high-demand
	periods when scheduling <code class="command">scrub</code> or use <a class="link" href="zfs-advanced.html#zfs-advanced-tuning-scrub_delay"><code class="varname">vfs.zfs.scrub_delay</code></a>
	to adjust the relative priority of the
	<code class="command">scrub</code> to prevent it interfering with other
	workloads.</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool scrub <em class="replaceable"><code>mypool</code></em></code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
  scan: scrub in progress since Wed Feb 19 20:52:54 2014
        116G scanned out of 8.60T at 649M/s, 3h48m to go
        0 repaired, 1.32% done
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          raidz2-0  ONLINE       0     0     0
            ada0p3  ONLINE       0     0     0
            ada1p3  ONLINE       0     0     0
            ada2p3  ONLINE       0     0     0
            ada3p3  ONLINE       0     0     0
            ada4p3  ONLINE       0     0     0
            ada5p3  ONLINE       0     0     0

errors: No known data errors</pre><p>In the event that a scrub operation needs to be cancelled,
	issue <code class="command">zpool scrub -s
	  <em class="replaceable"><code>mypool</code></em></code>.</p></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-selfheal"></a>19.3.8. Self-Healing</h3></div></div></div><p>The checksums stored with data blocks enable the file
	system to <span class="emphasis"><em>self-heal</em></span>.  This feature will
	automatically repair data whose checksum does not match the
	one recorded on another device that is part of the storage
	pool.  For example, a mirror with two disks where one drive is
	starting to malfunction and cannot properly store the data any
	more.  This is even worse when the data has not been accessed
	for a long time, as with long term archive storage.
	Traditional file systems need to run algorithms that check and
	repair the data like <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">fsck</span>(8)</span></a>.  These commands take time,
	and in severe cases, an administrator has to manually decide
	which repair operation must be performed.  When
	<acronym class="acronym">ZFS</acronym> detects a data block with a checksum
	that does not match, it tries to read the data from the mirror
	disk.  If that disk can provide the correct data, it will not
	only give that data to the application requesting it, but also
	correct the wrong data on the disk that had the bad checksum.
	This happens without any interaction from a system
	administrator during normal pool operation.</p><p>The next example demonstrates this self-healing behavior.
	A mirrored pool of disks <code class="filename">/dev/ada0</code> and
	<code class="filename">/dev/ada1</code> is created.</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool create <em class="replaceable"><code>healer</code></em> mirror <em class="replaceable"><code>/dev/ada0</code></em> <em class="replaceable"><code>/dev/ada1</code></em></code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zpool status <em class="replaceable"><code>healer</code></em></code></strong>
  pool: healer
 state: ONLINE
  scan: none requested
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0     0

errors: No known data errors
<code class="prompt">#</code> <strong class="userinput"><code>zpool list</code></strong>
NAME     SIZE  ALLOC   FREE   CKPOINT  EXPANDSZ   FRAG   CAP  DEDUP  HEALTH  ALTROOT
healer   960M  92.5K   960M         -         -     0%    0%  1.00x  ONLINE  -</pre><p>Some important data that to be protected from data errors
	using the self-healing feature is copied to the pool.  A
	checksum of the pool is created for later comparison.</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>cp /some/important/data /healer</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zfs list</code></strong>
NAME     SIZE  ALLOC   FREE    CAP  DEDUP  HEALTH  ALTROOT
healer   960M  67.7M   892M     7%  1.00x  ONLINE  -
<code class="prompt">#</code> <strong class="userinput"><code>sha1 /healer &gt; checksum.txt</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>cat checksum.txt</code></strong>
SHA1 (/healer) = 2753eff56d77d9a536ece6694bf0a82740344d1f</pre><p>Data corruption is simulated by writing random data to the
	beginning of one of the disks in the mirror.  To prevent
	<acronym class="acronym">ZFS</acronym> from healing the data as soon as it is
	detected, the pool is exported before the corruption and
	imported again afterwards.</p><div xmlns="" class="warning"><h3 class="admontitle">Warning: </h3><p xmlns="http://www.w3.org/1999/xhtml">This is a dangerous operation that can destroy vital
	  data.  It is shown here for demonstrational purposes only
	  and should not be attempted during normal operation of a
	  storage pool.  Nor should this intentional corruption
	  example be run on any disk with a different file system on
	  it.  Do not use any other disk device names other than the
	  ones that are part of the pool.  Make certain that proper
	  backups of the pool are created before running the
	  command!</p></div><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool export <em class="replaceable"><code>healer</code></em></code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>dd if=/dev/random of=/dev/ada1 bs=1m count=200</code></strong>
200+0 records in
200+0 records out
209715200 bytes transferred in 62.992162 secs (3329227 bytes/sec)
<code class="prompt">#</code> <strong class="userinput"><code>zpool import healer</code></strong></pre><p>The pool status shows that one device has experienced an
	error.  Note that applications reading data from the pool did
	not receive any incorrect data.  <acronym class="acronym">ZFS</acronym>
	provided data from the <code class="filename">ada0</code> device with
	the correct checksums.  The device with the wrong checksum can
	be found easily as the <code class="literal">CKSUM</code> column
	contains a nonzero value.</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool status <em class="replaceable"><code>healer</code></em></code></strong>
    pool: healer
   state: ONLINE
  status: One or more devices has experienced an unrecoverable error.  An
          attempt was made to correct the error.  Applications are unaffected.
  action: Determine if the device needs to be replaced, and clear the errors
          using 'zpool clear' or replace the device with 'zpool replace'.
     see: http://illumos.org/msg/ZFS-8000-4J
    scan: none requested
  config:

      NAME        STATE     READ WRITE CKSUM
      healer      ONLINE       0     0     0
        mirror-0  ONLINE       0     0     0
         ada0     ONLINE       0     0     0
         ada1     ONLINE       0     0     1

errors: No known data errors</pre><p>The error was detected and handled by using the redundancy
	present in the unaffected <code class="filename">ada0</code> mirror
	disk.  A checksum comparison with the original one will reveal
	whether the pool is consistent again.</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>sha1 /healer &gt;&gt; checksum.txt</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>cat checksum.txt</code></strong>
SHA1 (/healer) = 2753eff56d77d9a536ece6694bf0a82740344d1f
SHA1 (/healer) = 2753eff56d77d9a536ece6694bf0a82740344d1f</pre><p>The two checksums that were generated before and after the
	intentional tampering with the pool data still match.  This
	shows how <acronym class="acronym">ZFS</acronym> is capable of detecting and
	correcting any errors automatically when the checksums differ.
	Note that this is only possible when there is enough
	redundancy present in the pool.  A pool consisting of a single
	device has no self-healing capabilities.  That is also the
	reason why checksums are so important in
	<acronym class="acronym">ZFS</acronym> and should not be disabled for any
	reason.  No <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">fsck</span>(8)</span></a> or similar file system consistency
	check program is required to detect and correct this and the
	pool was still available during the time there was a problem.
	A scrub operation is now required to overwrite the corrupted
	data on <code class="filename">ada1</code>.</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool scrub <em class="replaceable"><code>healer</code></em></code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zpool status <em class="replaceable"><code>healer</code></em></code></strong>
  pool: healer
 state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
            attempt was made to correct the error.  Applications are unaffected.
action: Determine if the device needs to be replaced, and clear the errors
            using 'zpool clear' or replace the device with 'zpool replace'.
   see: http://illumos.org/msg/ZFS-8000-4J
  scan: scrub in progress since Mon Dec 10 12:23:30 2012
        10.4M scanned out of 67.0M at 267K/s, 0h3m to go
        9.63M repaired, 15.56% done
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0   627  (repairing)

errors: No known data errors</pre><p>The scrub operation reads data from
	<code class="filename">ada0</code> and rewrites any data with an
	incorrect checksum on <code class="filename">ada1</code>.  This is
	indicated by the <code class="literal">(repairing)</code> output from
	<code class="command">zpool status</code>.  After the operation is
	complete, the pool status changes to:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool status <em class="replaceable"><code>healer</code></em></code></strong>
  pool: healer
 state: ONLINE
status: One or more devices has experienced an unrecoverable error.  An
        attempt was made to correct the error.  Applications are unaffected.
action: Determine if the device needs to be replaced, and clear the errors
             using 'zpool clear' or replace the device with 'zpool replace'.
   see: http://illumos.org/msg/ZFS-8000-4J
  scan: scrub repaired 66.5M in 0h2m with 0 errors on Mon Dec 10 12:26:25 2012
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0 2.72K

errors: No known data errors</pre><p>After the scrub operation completes and all the data
	has been synchronized from <code class="filename">ada0</code> to
	<code class="filename">ada1</code>, the error messages can be
	<a class="link" href="zfs-zpool.html#zfs-zpool-clear" title="19.3.4. Clearing Errors">cleared</a> from the pool
	status by running <code class="command">zpool clear</code>.</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool clear <em class="replaceable"><code>healer</code></em></code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zpool status <em class="replaceable"><code>healer</code></em></code></strong>
  pool: healer
 state: ONLINE
  scan: scrub repaired 66.5M in 0h2m with 0 errors on Mon Dec 10 12:26:25 2012
config:

    NAME        STATE     READ WRITE CKSUM
    healer      ONLINE       0     0     0
      mirror-0  ONLINE       0     0     0
       ada0     ONLINE       0     0     0
       ada1     ONLINE       0     0     0

errors: No known data errors</pre><p>The pool is now back to a fully working state and all the
	errors have been cleared.</p></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-online"></a>19.3.9. Growing a Pool</h3></div></div></div><p>The usable size of a redundant pool is limited by the
	capacity of the smallest device in each vdev.  The smallest
	device can be replaced with a larger device.  After completing
	a <a class="link" href="zfs-zpool.html#zfs-zpool-replace" title="19.3.5. Replacing a Functioning Device">replace</a> or
	<a class="link" href="zfs-term.html#zfs-term-resilver">resilver</a> operation,
	the pool can grow to use the capacity of the new device.  For
	example, consider a mirror of a 1 TB drive and a
	2 TB drive.  The usable space is 1 TB.  When the
	1 TB drive is replaced with another 2 TB drive, the
	resilvering process copies the existing data onto the new
	drive.  Because
	both of the devices now have 2 TB capacity, the mirror's
	available space can be grown to 2 TB.</p><p>Expansion is triggered by using
	<code class="command">zpool online -e</code> on each device.  After
	expansion of all devices, the additional space becomes
	available to the pool.</p></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-import"></a>19.3.10. Importing and Exporting Pools</h3></div></div></div><p>Pools are <span class="emphasis"><em>exported</em></span> before moving them
	to another system.  All datasets are unmounted, and each
	device is marked as exported but still locked so it cannot be
	used by other disk subsystems.  This allows pools to be
	<span class="emphasis"><em>imported</em></span> on other machines, other
	operating systems that support <acronym class="acronym">ZFS</acronym>, and
	even different hardware architectures (with some caveats, see
	<a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=zpool&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">zpool</span>(8)</span></a>).  When a dataset has open files,
	<code class="command">zpool export -f</code> can be used to force the
	export of a pool.  Use this with caution.  The datasets are
	forcibly unmounted, potentially resulting in unexpected
	behavior by the applications which had open files on those
	datasets.</p><p>Export a pool that is not in use:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool export mypool</code></strong></pre><p>Importing a pool automatically mounts the datasets.  This
	may not be the desired behavior, and can be prevented with
	<code class="command">zpool import -N</code>.
	<code class="command">zpool import -o</code> sets temporary properties
	for this import only.
	<code class="command">zpool import altroot=</code> allows importing a
	pool with a base mount point instead of the root of the file
	system.  If the pool was last used on a different system and
	was not properly exported, an import might have to be forced
	with <code class="command">zpool import -f</code>.
	<code class="command">zpool import -a</code> imports all pools that do
	not appear to be in use by another system.</p><p>List all available pools for import:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool import</code></strong>
   pool: mypool
     id: 9930174748043525076
  state: ONLINE
 action: The pool can be imported using its name or numeric identifier.
 config:

        mypool      ONLINE
          ada2p3    ONLINE</pre><p>Import the pool with an alternative root directory:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool import -o altroot=<em class="replaceable"><code>/mnt</code></em> <em class="replaceable"><code>mypool</code></em></code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zfs list</code></strong>
zfs list
NAME                 USED  AVAIL  REFER  MOUNTPOINT
mypool               110K  47.0G    31K  /mnt/mypool</pre></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-upgrade"></a>19.3.11. Upgrading a Storage Pool</h3></div></div></div><p>After upgrading FreeBSD, or if a pool has been imported from
	a system using an older version of <acronym class="acronym">ZFS</acronym>, the
	pool can be manually upgraded to the latest version of
	<acronym class="acronym">ZFS</acronym> to support newer features.  Consider
	whether the pool may ever need to be imported on an older
	system before upgrading.  Upgrading is a one-way process.
	Older pools can be upgraded, but pools with newer features
	cannot be downgraded.</p><p>Upgrade a v28 pool to support
	<code class="literal">Feature Flags</code>:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
status: The pool is formatted using a legacy on-disk format.  The pool can
        still be used, but some features are unavailable.
action: Upgrade the pool using 'zpool upgrade'.  Once this is done, the
        pool will no longer be accessible on software that does not support feat
        flags.
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
	    ada0    ONLINE       0     0     0
	    ada1    ONLINE       0     0     0

errors: No known data errors
<code class="prompt">#</code> <strong class="userinput"><code>zpool upgrade</code></strong>
This system supports ZFS pool feature flags.

The following pools are formatted with legacy version numbers and can
be upgraded to use feature flags.  After being upgraded, these pools
will no longer be accessible by software that does not support feature
flags.

VER  POOL
---  ------------
28   mypool

Use 'zpool upgrade -v' for a list of available legacy versions.
Every feature flags pool has all supported features enabled.
<code class="prompt">#</code> <strong class="userinput"><code>zpool upgrade mypool</code></strong>
This system supports ZFS pool feature flags.

Successfully upgraded 'mypool' from version 28 to feature flags.
Enabled the following features on 'mypool':
  async_destroy
  empty_bpobj
  lz4_compress
  multi_vdev_crash_dump</pre><p>The newer features of <acronym class="acronym">ZFS</acronym> will not be
	available until <code class="command">zpool upgrade</code> has
	completed.  <code class="command">zpool upgrade -v</code> can be used to
	see what new features will be provided by upgrading, as well
	as which features are already supported.</p><p>Upgrade a pool to support additional feature flags:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool status</code></strong>
  pool: mypool
 state: ONLINE
status: Some supported features are not enabled on the pool. The pool can
        still be used, but some features are unavailable.
action: Enable all features using 'zpool upgrade'. Once this is done,
        the pool may no longer be accessible by software that does not support
        the features. See zpool-features(7) for details.
  scan: none requested
config:

        NAME        STATE     READ WRITE CKSUM
        mypool      ONLINE       0     0     0
          mirror-0  ONLINE       0     0     0
	    ada0    ONLINE       0     0     0
	    ada1    ONLINE       0     0     0

errors: No known data errors
<code class="prompt">#</code> <strong class="userinput"><code>zpool upgrade</code></strong>
This system supports ZFS pool feature flags.

All pools are formatted using feature flags.


Some supported features are not enabled on the following pools. Once a
feature is enabled the pool may become incompatible with software
that does not support the feature. See zpool-features(7) for details.

POOL  FEATURE
---------------
zstore
      multi_vdev_crash_dump
      spacemap_histogram
      enabled_txg
      hole_birth
      extensible_dataset
      bookmarks
      filesystem_limits
<code class="prompt">#</code> <strong class="userinput"><code>zpool upgrade mypool</code></strong>
This system supports ZFS pool feature flags.

Enabled the following features on 'mypool':
  spacemap_histogram
  enabled_txg
  hole_birth
  extensible_dataset
  bookmarks
  filesystem_limits</pre><div xmlns="" class="warning"><h3 class="admontitle">Warning: </h3><p xmlns="http://www.w3.org/1999/xhtml">The boot code on systems that boot from a pool must be
	  updated to support the new pool version.  Use
	  <code class="command">gpart bootcode</code> on the partition that
	  contains the boot code.  There are two types of bootcode
	  available, depending on way the system boots:
	  <acronym class="acronym">GPT</acronym> (the most common option) and
	  <acronym class="acronym">EFI</acronym> (for more modern systems).</p><p xmlns="http://www.w3.org/1999/xhtml">For legacy boot using GPT, use the following
	  command:</p><pre xmlns="http://www.w3.org/1999/xhtml" class="screen"><code class="prompt">#</code> <strong class="userinput"><code>gpart bootcode -b /boot/pmbr -p /boot/gptzfsboot -i <em class="replaceable"><code>1</code></em> <em class="replaceable"><code>ada1</code></em></code></strong></pre><p xmlns="http://www.w3.org/1999/xhtml">For systems using EFI to boot, execute the following
	  command:</p><pre xmlns="http://www.w3.org/1999/xhtml" class="screen"><code class="prompt">#</code> <strong class="userinput"><code>gpart bootcode -p /boot/boot1.efifat -i <em class="replaceable"><code>1</code></em> <em class="replaceable"><code>ada1</code></em></code></strong></pre><p xmlns="http://www.w3.org/1999/xhtml">Apply the bootcode to all bootable disks in the pool.
	  See <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=gpart&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">gpart</span>(8)</span></a> for more information.</p></div></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-history"></a>19.3.12. Displaying Recorded Pool History</h3></div></div></div><p>Commands that modify the pool are recorded.  Recorded
	actions include the creation of datasets, changing properties,
	or replacement of a disk.  This history is useful for
	reviewing how a pool was created and which user performed a
	specific action and when.  History is not kept in a log file,
	but is part of the pool itself.  The command to review this
	history is aptly named
	<code class="command">zpool history</code>:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool history</code></strong>
History for 'tank':
2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1
2013-02-27.18:50:58 zfs set atime=off tank
2013-02-27.18:51:09 zfs set checksum=fletcher4 tank
2013-02-27.18:51:18 zfs create tank/backup</pre><p>The output shows <code class="command">zpool</code> and
	<code class="command">zfs</code> commands that were executed on the pool
	along with a timestamp.  Only commands that alter the pool in
	some way are recorded.  Commands like
	<code class="command">zfs list</code> are not included.  When no pool
	name is specified, the history of all pools is
	displayed.</p><p><code class="command">zpool history</code> can show even more
	information when the options <code class="option">-i</code> or
	<code class="option">-l</code> are provided.  <code class="option">-i</code>
	displays user-initiated events as well as internally logged
	<acronym class="acronym">ZFS</acronym> events.</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool history -i</code></strong>
History for 'tank':
2013-02-26.23:02:35 [internal pool create txg:5] pool spa 28; zfs spa 28; zpl 5;uts  9.1-RELEASE 901000 amd64
2013-02-27.18:50:53 [internal property set txg:50] atime=0 dataset = 21
2013-02-27.18:50:58 zfs set atime=off tank
2013-02-27.18:51:04 [internal property set txg:53] checksum=7 dataset = 21
2013-02-27.18:51:09 zfs set checksum=fletcher4 tank
2013-02-27.18:51:13 [internal create txg:55] dataset = 39
2013-02-27.18:51:18 zfs create tank/backup</pre><p>More details can be shown by adding <code class="option">-l</code>.
	History records are shown in a long format, including
	information like the name of the user who issued the command
	and the hostname on which the change was made.</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool history -l</code></strong>
History for 'tank':
2013-02-26.23:02:35 zpool create tank mirror /dev/ada0 /dev/ada1 [user 0 (root) on :global]
2013-02-27.18:50:58 zfs set atime=off tank [user 0 (root) on myzfsbox:global]
2013-02-27.18:51:09 zfs set checksum=fletcher4 tank [user 0 (root) on myzfsbox:global]
2013-02-27.18:51:18 zfs create tank/backup [user 0 (root) on myzfsbox:global]</pre><p>The output shows that the
	<code class="systemitem">root</code> user created
	the mirrored pool with disks
	<code class="filename">/dev/ada0</code> and
	<code class="filename">/dev/ada1</code>.  The hostname
	<code class="systemitem">myzfsbox</code> is also
	shown in the commands after the pool's creation.  The hostname
	display becomes important when the pool is exported from one
	system and imported on another.  The commands that are issued
	on the other system can clearly be distinguished by the
	hostname that is recorded for each command.</p><p>Both options to <code class="command">zpool history</code> can be
	combined to give the most detailed information possible for
	any given pool.  Pool history provides valuable information
	when tracking down the actions that were performed or when
	more detailed output is needed for debugging.</p></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-iostat"></a>19.3.13. Performance Monitoring</h3></div></div></div><p>A built-in monitoring system can display pool
	<acronym class="acronym">I/O</acronym> statistics in real time.  It shows the
	amount of free and used space on the pool, how many read and
	write operations are being performed per second, and how much
	<acronym class="acronym">I/O</acronym> bandwidth is currently being utilized.
	By default, all pools in the system are monitored and
	displayed.  A pool name can be provided to limit monitoring to
	just that pool.  A basic example:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool iostat</code></strong>
               capacity     operations    bandwidth
pool        alloc   free   read  write   read  write
----------  -----  -----  -----  -----  -----  -----
data         288G  1.53T      2     11  11.3K  57.1K</pre><p>To continuously monitor <acronym class="acronym">I/O</acronym> activity, a
	number can be specified as the last parameter, indicating a
	interval in seconds to wait between updates.  The next
	statistic line is printed after each interval.  Press
	<span class="keycap"><strong>Ctrl</strong></span>+<span class="keycap"><strong>C</strong></span> to stop this continuous monitoring.
	Alternatively, give a second number on the command line after
	the interval to specify the total number of statistics to
	display.</p><p>Even more detailed <acronym class="acronym">I/O</acronym> statistics can
	be displayed with <code class="option">-v</code>.  Each device in the
	pool is shown with a statistics line.  This is useful in
	seeing how many read and write operations are being performed
	on each device, and can help determine if any individual
	device is slowing down the pool.  This example shows a
	mirrored pool with two devices:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool iostat -v </code></strong>
                            capacity     operations    bandwidth
pool                     alloc   free   read  write   read  write
-----------------------  -----  -----  -----  -----  -----  -----
data                      288G  1.53T      2     12  9.23K  61.5K
  mirror                  288G  1.53T      2     12  9.23K  61.5K
    ada1                     -      -      0      4  5.61K  61.7K
    ada2                     -      -      1      4  5.04K  61.7K
-----------------------  -----  -----  -----  -----  -----  -----</pre></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-zpool-split"></a>19.3.14. Splitting a Storage Pool</h3></div></div></div><p>A pool consisting of one or more mirror vdevs can be split
	into two pools.  Unless otherwise specified, the last member
	of each mirror is detached and used to create a new pool
	containing the same data.  The operation should first be
	attempted with <code class="option">-n</code>.  The details of the
	proposed operation are displayed without it actually being
	performed.  This helps confirm that the operation will do what
	the user intends.</p></div></div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="zfs-quickstart.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="zfs.html">Up</a></td><td width="40%" align="right"> <a accesskey="n" href="zfs-zfs.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">19.2. Quick Start Guide </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> 19.4. <code class="command">zfs</code> Administration</td></tr></table></div><p xmlns="" align="center"><small>All FreeBSD documents are available for download
    at <a href="https://download.freebsd.org/ftp/doc/">https://download.freebsd.org/ftp/doc/</a></small></p><p xmlns="" align="center"><small>Questions that are not answered by the
    <a href="https://www.FreeBSD.org/docs.html">documentation</a> may be
    sent to &lt;<a href="mailto:freebsd-questions@FreeBSD.org">freebsd-questions@FreeBSD.org</a>&gt;.<br />
    Send questions about this document to &lt;<a href="mailto:freebsd-doc@FreeBSD.org">freebsd-doc@FreeBSD.org</a>&gt;.</small></p></body></html>