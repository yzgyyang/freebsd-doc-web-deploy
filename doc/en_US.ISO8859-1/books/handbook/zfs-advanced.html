<?xml version="1.0" encoding="iso-8859-1" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" /><title>19.6. Advanced Topics</title><link rel="stylesheet" type="text/css" href="docbook.css" /><link rev="made" href="mailto:doc@FreeBSD.org" /><meta name="generator" content="DocBook XSL Stylesheets V1.79.1" /><link rel="home" href="index.html" title="FreeBSD Handbook" /><link rel="up" href="zfs.html" title="Chapter 19. The Z File System (ZFS)" /><link rel="prev" href="zfs-zfs-allow.html" title="19.5. Delegated Administration" /><link rel="next" href="zfs-links.html" title="19.7. Additional Resources" /><link rel="copyright" href="legalnotice.html" title="Copyright" /><script xmlns="" type="text/javascript" src="/layout/js/google.js"></script></head><body><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">19.6. Advanced Topics</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="zfs-zfs-allow.html">Prev</a> </td><th width="60%" align="center">Chapter 19. The Z File System (<acronym class="acronym">ZFS</acronym>)</th><td width="20%" align="right"> <a accesskey="n" href="zfs-links.html">Next</a></td></tr></table><hr /></div><div class="sect1"><div xmlns="" class="titlepage"><div><div><h2 xmlns="http://www.w3.org/1999/xhtml" class="title" style="clear: both"><a id="zfs-advanced"></a>19.6. Advanced Topics</h2></div></div></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-advanced-tuning"></a>19.6.1. Tuning</h3></div></div></div><p>There are a number of tunables that can be adjusted to
	make <acronym class="acronym">ZFS</acronym> perform best for different
	workloads.</p><div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><a id="zfs-advanced-tuning-arc_max"></a><span class="emphasis"><em><code class="varname">vfs.zfs.arc_max</code></em></span>
	    - Maximum size of the <a class="link" href="zfs-term.html#zfs-term-arc"><acronym class="acronym">ARC</acronym></a>.
	    The default is all <acronym class="acronym">RAM</acronym> but 1 GB,
	    or 5/8 of all <acronym class="acronym">RAM</acronym>, whichever is more.
	    However, a lower value should be used if the system will
	    be running any other daemons or processes that may require
	    memory.  This value can be adjusted at runtime with
	    <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a> and can be set in
	    <code class="filename">/boot/loader.conf</code> or
	    <code class="filename">/etc/sysctl.conf</code>.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-arc_meta_limit"></a><span class="emphasis"><em><code class="varname">vfs.zfs.arc_meta_limit</code></em></span>
	    - Limit the portion of the
	    <a class="link" href="zfs-term.html#zfs-term-arc"><acronym class="acronym">ARC</acronym></a>
	    that can be used to store metadata.  The default is one
	    fourth of <code class="varname">vfs.zfs.arc_max</code>.  Increasing
	    this value will improve performance if the workload
	    involves operations on a large number of files and
	    directories, or frequent metadata operations, at the cost
	    of less file data fitting in the <a class="link" href="zfs-term.html#zfs-term-arc"><acronym class="acronym">ARC</acronym></a>.
	    This value can be adjusted at runtime with <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a>
	    and can be set in
	    <code class="filename">/boot/loader.conf</code> or
	    <code class="filename">/etc/sysctl.conf</code>.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-arc_min"></a><span class="emphasis"><em><code class="varname">vfs.zfs.arc_min</code></em></span>
	    - Minimum size of the <a class="link" href="zfs-term.html#zfs-term-arc"><acronym class="acronym">ARC</acronym></a>.
	    The default is one half of
	    <code class="varname">vfs.zfs.arc_meta_limit</code>.  Adjust this
	    value to prevent other applications from pressuring out
	    the entire <a class="link" href="zfs-term.html#zfs-term-arc"><acronym class="acronym">ARC</acronym></a>.
	    This value can be adjusted at runtime with <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a>
	    and can be set in
	    <code class="filename">/boot/loader.conf</code> or
	    <code class="filename">/etc/sysctl.conf</code>.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-vdev-cache-size"></a><span class="emphasis"><em><code class="varname">vfs.zfs.vdev.cache.size</code></em></span>
	    - A preallocated amount of memory reserved as a cache for
	    each device in the pool.  The total amount of memory used
	    will be this value multiplied by the number of devices.
	    This value can only be adjusted at boot time, and is set
	    in <code class="filename">/boot/loader.conf</code>.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-min-auto-ashift"></a><span class="emphasis"><em><code class="varname">vfs.zfs.min_auto_ashift</code></em></span>
	    - Minimum <code class="varname">ashift</code> (sector size) that
	    will be used automatically at pool creation time.  The
	    value is a power of two.  The default value of
	    <code class="literal">9</code> represents
	    <code class="literal">2^9 = 512</code>, a sector size of 512 bytes.
	    To avoid <span class="emphasis"><em>write amplification</em></span> and get
	    the best performance, set this value to the largest sector
	    size used by a device in the pool.</p><p>Many drives have 4 KB sectors.  Using the default
	    <code class="varname">ashift</code> of <code class="literal">9</code> with
	    these drives results in write amplification on these
	    devices.  Data that could be contained in a single
	    4 KB write must instead be written in eight 512-byte
	    writes.  <acronym class="acronym">ZFS</acronym> tries to read the native
	    sector size from all devices when creating a pool, but
	    many drives with 4 KB sectors report that their
	    sectors are 512 bytes for compatibility.  Setting
	    <code class="varname">vfs.zfs.min_auto_ashift</code> to
	    <code class="literal">12</code> (<code class="literal">2^12 = 4096</code>)
	    before creating a pool forces <acronym class="acronym">ZFS</acronym> to
	    use 4 KB blocks for best performance on these
	    drives.</p><p>Forcing 4 KB blocks is also useful on pools where
	    disk upgrades are planned.  Future disks are likely to use
	    4 KB sectors, and <code class="varname">ashift</code> values
	    cannot be changed after a pool is created.</p><p>In some specific cases, the smaller 512-byte block
	    size might be preferable.  When used with 512-byte disks
	    for databases, or as storage for virtual machines, less
	    data is transferred during small random reads.  This can
	    provide better performance, especially when using a
	    smaller <acronym class="acronym">ZFS</acronym> record size.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-prefetch_disable"></a><span class="emphasis"><em><code class="varname">vfs.zfs.prefetch_disable</code></em></span>
	    - Disable prefetch.  A value of <code class="literal">0</code> is
	    enabled and <code class="literal">1</code> is disabled.  The default
	    is <code class="literal">0</code>, unless the system has less than
	    4 GB of <acronym class="acronym">RAM</acronym>.  Prefetch works by
	    reading larger blocks than were requested into the
	    <a class="link" href="zfs-term.html#zfs-term-arc"><acronym class="acronym">ARC</acronym></a>
	    in hopes that the data will be needed soon.  If the
	    workload has a large number of random reads, disabling
	    prefetch may actually improve performance by reducing
	    unnecessary reads.  This value can be adjusted at any time
	    with <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a>.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-vdev-trim_on_init"></a><span class="emphasis"><em><code class="varname">vfs.zfs.vdev.trim_on_init</code></em></span>
	    - Control whether new devices added to the pool have the
	    <code class="literal">TRIM</code> command run on them.  This ensures
	    the best performance and longevity for
	    <acronym class="acronym">SSD</acronym>s, but takes extra time.  If the
	    device has already been secure erased, disabling this
	    setting will make the addition of the new device faster.
	    This value can be adjusted at any time with
	    <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a>.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-vdev-max_pending"></a><span class="emphasis"><em><code class="varname">vfs.zfs.vdev.max_pending</code></em></span>
	    - Limit the number of pending I/O requests per device.
	    A higher value will keep the device command queue full
	    and may give higher throughput.  A lower value will reduce
	    latency.  This value can be adjusted at any time with
	    <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a>.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-top_maxinflight"></a><span class="emphasis"><em><code class="varname">vfs.zfs.top_maxinflight</code></em></span>
	    - Maxmimum number of outstanding I/Os per top-level
	    <a class="link" href="zfs-term.html#zfs-term-vdev">vdev</a>.  Limits the
	    depth of the command queue to prevent high latency.  The
	    limit is per top-level vdev, meaning the limit applies to
	    each <a class="link" href="zfs-term.html#zfs-term-vdev-mirror">mirror</a>,
	    <a class="link" href="zfs-term.html#zfs-term-vdev-raidz">RAID-Z</a>, or
	    other vdev independently.  This value can be adjusted at
	    any time with <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a>.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-l2arc_write_max"></a><span class="emphasis"><em><code class="varname">vfs.zfs.l2arc_write_max</code></em></span>
	    - Limit the amount of data written to the <a class="link" href="zfs-term.html#zfs-term-l2arc"><acronym class="acronym">L2ARC</acronym></a>
	    per second.  This tunable is designed to extend the
	    longevity of <acronym class="acronym">SSD</acronym>s by limiting the
	    amount of data written to the device.  This value can be
	    adjusted at any time with <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a>.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-l2arc_write_boost"></a><span class="emphasis"><em><code class="varname">vfs.zfs.l2arc_write_boost</code></em></span>
	    - The value of this tunable is added to <a class="link" href="zfs-advanced.html#zfs-advanced-tuning-l2arc_write_max"><code class="varname">vfs.zfs.l2arc_write_max</code></a>
	    and increases the write speed to the
	    <acronym class="acronym">SSD</acronym> until the first block is evicted
	    from the <a class="link" href="zfs-term.html#zfs-term-l2arc"><acronym class="acronym">L2ARC</acronym></a>.
	    This <span class="quote">&#8220;<span class="quote">Turbo Warmup Phase</span>&#8221;</span> is designed to
	    reduce the performance loss from an empty <a class="link" href="zfs-term.html#zfs-term-l2arc"><acronym class="acronym">L2ARC</acronym></a>
	    after a reboot.  This value can be adjusted at any time
	    with <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a>.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-scrub_delay"></a><span class="emphasis"><em><code class="varname">vfs.zfs.scrub_delay</code></em></span>
	    - Number of ticks to delay between each I/O during a
	    <a class="link" href="zfs-term.html#zfs-term-scrub"><code class="command">scrub</code></a>.
	    To ensure that a <code class="command">scrub</code> does not
	    interfere with the normal operation of the pool, if any
	    other <acronym class="acronym">I/O</acronym> is happening the
	    <code class="command">scrub</code> will delay between each command.
	    This value controls the limit on the total
	    <acronym class="acronym">IOPS</acronym> (I/Os Per Second) generated by the
	    <code class="command">scrub</code>.  The granularity of the setting
	    is determined by the value of <code class="varname">kern.hz</code>
	    which defaults to 1000 ticks per second.  This setting may
	    be changed, resulting in a different effective
	    <acronym class="acronym">IOPS</acronym> limit.  The default value is
	    <code class="literal">4</code>, resulting in a limit of:
	    1000 ticks/sec / 4 =
	    250 <acronym class="acronym">IOPS</acronym>.  Using a value of
	    <em class="replaceable"><code>20</code></em> would give a limit of:
	    1000 ticks/sec / 20 =
	    50 <acronym class="acronym">IOPS</acronym>.  The speed of
	    <code class="command">scrub</code> is only limited when there has
	    been recent activity on the pool, as determined by <a class="link" href="zfs-advanced.html#zfs-advanced-tuning-scan_idle"><code class="varname">vfs.zfs.scan_idle</code></a>.
	    This value can be adjusted at any time with
	    <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a>.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-resilver_delay"></a><span class="emphasis"><em><code class="varname">vfs.zfs.resilver_delay</code></em></span>
	    - Number of milliseconds of delay inserted between
	    each I/O during a
	    <a class="link" href="zfs-term.html#zfs-term-resilver">resilver</a>.  To
	    ensure that a resilver does not interfere with the normal
	    operation of the pool, if any other I/O is happening the
	    resilver will delay between each command.  This value
	    controls the limit of total <acronym class="acronym">IOPS</acronym> (I/Os
	    Per Second) generated by the resilver.  The granularity of
	    the setting is determined by the value of
	    <code class="varname">kern.hz</code> which defaults to 1000 ticks
	    per second.  This setting may be changed, resulting in a
	    different effective <acronym class="acronym">IOPS</acronym> limit.  The
	    default value is 2, resulting in a limit of:
	    1000 ticks/sec / 2 =
	    500 <acronym class="acronym">IOPS</acronym>.  Returning the pool to
	    an <a class="link" href="zfs-term.html#zfs-term-online">Online</a> state may
	    be more important if another device failing could
	    <a class="link" href="zfs-term.html#zfs-term-faulted">Fault</a> the pool,
	    causing data loss.  A value of 0 will give the resilver
	    operation the same priority as other operations, speeding
	    the healing process.  The speed of resilver is only
	    limited when there has been other recent activity on the
	    pool, as determined by <a class="link" href="zfs-advanced.html#zfs-advanced-tuning-scan_idle"><code class="varname">vfs.zfs.scan_idle</code></a>.
	    This value can be adjusted at any time with
	    <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a>.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-scan_idle"></a><span class="emphasis"><em><code class="varname">vfs.zfs.scan_idle</code></em></span>
	    - Number of milliseconds since the last operation before
	    the pool is considered idle.  When the pool is idle the
	    rate limiting for <a class="link" href="zfs-term.html#zfs-term-scrub"><code class="command">scrub</code></a>
	    and
	    <a class="link" href="zfs-term.html#zfs-term-resilver">resilver</a> are
	    disabled.  This value can be adjusted at any time with
	    <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a>.</p></li><li class="listitem"><p><a id="zfs-advanced-tuning-txg-timeout"></a><span class="emphasis"><em><code class="varname">vfs.zfs.txg.timeout</code></em></span>
	    - Maximum number of seconds between
	    <a class="link" href="zfs-term.html#zfs-term-txg">transaction group</a>s.
	    The current transaction group will be written to the pool
	    and a fresh transaction group started if this amount of
	    time has elapsed since the previous transaction group.  A
	    transaction group my be triggered earlier if enough data
	    is written.  The default value is 5 seconds.  A larger
	    value may improve read performance by delaying
	    asynchronous writes, but this may cause uneven performance
	    when the transaction group is written.  This value can be
	    adjusted at any time with <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a>.</p></li></ul></div></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-advanced-i386"></a>19.6.2. <acronym class="acronym">ZFS</acronym> on i386</h3></div></div></div><p>Some of the features provided by <acronym class="acronym">ZFS</acronym>
	are memory intensive, and may require tuning for maximum
	efficiency on systems with limited
	<acronym class="acronym">RAM</acronym>.</p><div class="sect3"><div xmlns="" class="titlepage"><div><div><h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp65053816"></a>19.6.2.1. Memory</h4></div></div></div><p>As a bare minimum, the total system memory should be at
	  least one gigabyte.  The amount of recommended
	  <acronym class="acronym">RAM</acronym> depends upon the size of the pool and
	  which <acronym class="acronym">ZFS</acronym> features are used.  A general
	  rule of thumb is 1 GB of RAM for every 1 TB of
	  storage.  If the deduplication feature is used, a general
	  rule of thumb is 5 GB of RAM per TB of storage to be
	  deduplicated.  While some users successfully use
	  <acronym class="acronym">ZFS</acronym> with less <acronym class="acronym">RAM</acronym>,
	  systems under heavy load may panic due to memory exhaustion.
	  Further tuning may be required for systems with less than
	  the recommended RAM requirements.</p></div><div class="sect3"><div xmlns="" class="titlepage"><div><div><h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp65060600"></a>19.6.2.2. Kernel Configuration</h4></div></div></div><p>Due to the address space limitations of the
	  <span class="trademark">i386</span>&#8482; platform, <acronym class="acronym">ZFS</acronym> users on the
	  <span class="trademark">i386</span>&#8482; architecture must add this option to a
	  custom kernel configuration file, rebuild the kernel, and
	  reboot:</p><pre class="programlisting">options        KVA_PAGES=512</pre><p>This expands the kernel address space, allowing
	  the <code class="varname">vm.kvm_size</code> tunable to be pushed
	  beyond the currently imposed limit of 1 GB, or the
	  limit of 2 GB for <acronym class="acronym">PAE</acronym>.  To find the
	  most suitable value for this option, divide the desired
	  address space in megabytes by four.  In this example, it
	  is <code class="literal">512</code> for 2 GB.</p></div><div class="sect3"><div xmlns="" class="titlepage"><div><div><h4 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="idp65068920"></a>19.6.2.3. Loader Tunables</h4></div></div></div><p>The <code class="filename">kmem</code> address space can be
	  increased on all FreeBSD architectures.  On a test system with
	  1 GB of physical memory, success was achieved with
	  these options added to
	  <code class="filename">/boot/loader.conf</code>, and the system
	  restarted:</p><pre class="programlisting">vm.kmem_size="330M"
vm.kmem_size_max="330M"
vfs.zfs.arc_max="40M"
vfs.zfs.vdev.cache.size="5M"</pre><p>For a more detailed list of recommendations for
	  <acronym class="acronym">ZFS</acronym>-related tuning, see <a class="link" href="https://wiki.freebsd.org/ZFSTuningGuide" target="_top">https://wiki.freebsd.org/ZFSTuningGuide</a>.</p></div></div></div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="zfs-zfs-allow.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="zfs.html">Up</a></td><td width="40%" align="right"> <a accesskey="n" href="zfs-links.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">19.5. Delegated Administration </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> 19.7. Additional Resources</td></tr></table></div><p xmlns="" align="center"><small>All FreeBSD documents are available for download
    at <a href="https://download.freebsd.org/ftp/doc/">https://download.freebsd.org/ftp/doc/</a></small></p><p xmlns="" align="center"><small>Questions that are not answered by the
    <a href="https://www.FreeBSD.org/docs.html">documentation</a> may be
    sent to &lt;<a href="mailto:freebsd-questions@FreeBSD.org">freebsd-questions@FreeBSD.org</a>&gt;.<br />
    Send questions about this document to &lt;<a href="mailto:freebsd-doc@FreeBSD.org">freebsd-doc@FreeBSD.org</a>&gt;.</small></p></body></html>