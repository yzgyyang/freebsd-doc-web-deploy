<?xml version="1.0" encoding="iso-8859-1" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" /><title>19.8. ZFS Features and Terminology</title><link rel="stylesheet" type="text/css" href="docbook.css" /><link rev="made" href="mailto:doc@FreeBSD.org" /><meta name="generator" content="DocBook XSL Stylesheets V1.79.1" /><link rel="home" href="index.html" title="FreeBSD Handbook" /><link rel="up" href="zfs.html" title="Chapter 19. The Z File System (ZFS)" /><link rel="prev" href="zfs-links.html" title="19.7. Additional Resources" /><link rel="next" href="filesystems.html" title="Chapter 20. Other File Systems" /><link rel="copyright" href="legalnotice.html" title="Copyright" /><script xmlns="" type="text/javascript" src="/layout/js/google.js"></script></head><body><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">19.8. <acronym class="acronym">ZFS</acronym> Features and Terminology</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="zfs-links.html">Prev</a> </td><th width="60%" align="center">Chapter 19. The Z File System (<acronym class="acronym">ZFS</acronym>)</th><td width="20%" align="right"> <a accesskey="n" href="filesystems.html">Next</a></td></tr></table><hr /></div><div class="sect1"><div xmlns="" class="titlepage"><div><div><h2 xmlns="http://www.w3.org/1999/xhtml" class="title" style="clear: both"><a id="zfs-term"></a>19.8. <acronym class="acronym">ZFS</acronym> Features and Terminology</h2></div></div></div><p><acronym class="acronym">ZFS</acronym> is a fundamentally different file
      system because it is more than just a file system.
      <acronym class="acronym">ZFS</acronym> combines the roles of file system and
      volume manager, enabling additional storage devices to be added
      to a live system and having the new space available on all of
      the existing file systems in that pool immediately.  By
      combining the traditionally separate roles,
      <acronym class="acronym">ZFS</acronym> is able to overcome previous limitations
      that prevented <acronym class="acronym">RAID</acronym> groups being able to
      grow.  Each top level device in a pool is called a
      <span class="emphasis"><em>vdev</em></span>, which can be a simple disk or a
      <acronym class="acronym">RAID</acronym> transformation such as a mirror or
      <acronym class="acronym">RAID-Z</acronym> array.  <acronym class="acronym">ZFS</acronym> file
      systems (called <span class="emphasis"><em>datasets</em></span>) each have access
      to the combined free space of the entire pool.  As blocks are
      allocated from the pool, the space available to each file system
      decreases.  This approach avoids the common pitfall with
      extensive partitioning where free space becomes fragmented
      across the partitions.</p><div class="informaltable"><table class="informaltable" width="100%" border="1"><colgroup><col /><col /></colgroup><tbody valign="top"><tr><td valign="top"><a id="zfs-term-pool"></a>pool</td><td valign="top">A storage <span class="emphasis"><em>pool</em></span> is the most
	      basic building block of <acronym class="acronym">ZFS</acronym>.  A pool
	      is made up of one or more vdevs, the underlying devices
	      that store the data.  A pool is then used to create one
	      or more file systems (datasets) or block devices
	      (volumes).  These datasets and volumes share the pool of
	      remaining free space.  Each pool is uniquely identified
	      by a name and a <acronym class="acronym">GUID</acronym>.  The features
	      available are determined by the <acronym class="acronym">ZFS</acronym>
	      version number on the pool.</td></tr><tr><td valign="top"><a id="zfs-term-vdev"></a>vdev Types</td><td valign="top">A pool is made up of one or more vdevs, which
	      themselves can be a single disk or a group of disks, in
	      the case of a <acronym class="acronym">RAID</acronym> transform.  When
	      multiple vdevs are used, <acronym class="acronym">ZFS</acronym> spreads
	      data across the vdevs to increase performance and
	      maximize usable space.

	      <div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><a id="zfs-term-vdev-disk"></a><span class="emphasis"><em>Disk</em></span>
		    - The most basic type of vdev is a standard block
		    device.  This can be an entire disk (such as
		    <code class="filename"><em class="replaceable"><code>/dev/ada0</code></em></code>
		    or
		    <code class="filename"><em class="replaceable"><code>/dev/da0</code></em></code>)
		    or a partition
		    (<code class="filename"><em class="replaceable"><code>/dev/ada0p3</code></em></code>).
		    On FreeBSD, there is no performance penalty for using
		    a partition rather than the entire disk.  This
		    differs from recommendations made by the Solaris
		    documentation.</p><div xmlns="" class="caution"><h3 class="admontitle">Caution: </h3><p xmlns="http://www.w3.org/1999/xhtml">Using an entire disk as part of a bootable
		      pool is strongly discouraged, as this may render
		      the pool unbootable.  Likewise, you should not
		      use an entire disk as part of a mirror or
		      <acronym class="acronym">RAID-Z</acronym> vdev.  These are
		      because it it impossible to reliably determine
		      the size of an unpartitioned disk at boot time
		      and because there's no place to put in boot
		      code.</p></div></li><li class="listitem"><p><a id="zfs-term-vdev-file"></a><span class="emphasis"><em>File</em></span>
		    - In addition to disks, <acronym class="acronym">ZFS</acronym>
		    pools can be backed by regular files, this is
		    especially useful for testing and experimentation.
		    Use the full path to the file as the device path
		    in <code class="command">zpool create</code>.  All vdevs
		    must be at least 128 MB in size.</p></li><li class="listitem"><p><a id="zfs-term-vdev-mirror"></a><span class="emphasis"><em>Mirror</em></span>
		    - When creating a mirror, specify the
		    <code class="literal">mirror</code> keyword followed by the
		    list of member devices for the mirror.  A mirror
		    consists of two or more devices, all data will be
		    written to all member devices.  A mirror vdev will
		    only hold as much data as its smallest member.  A
		    mirror vdev can withstand the failure of all but
		    one of its members without losing any data.</p><div xmlns="" class="note"><h3 class="admontitle">Note: </h3><p xmlns="http://www.w3.org/1999/xhtml">A regular single disk vdev can be upgraded
		      to a mirror vdev at any time with
		      <code class="command">zpool
			<a class="link" href="zfs-zpool.html#zfs-zpool-attach" title="19.3.2. Adding and Removing Devices">attach</a></code>.</p></div></li><li class="listitem"><p><a id="zfs-term-vdev-raidz"></a><span class="emphasis"><em><acronym class="acronym">RAID-Z</acronym></em></span>
		    - <acronym class="acronym">ZFS</acronym> implements
		    <acronym class="acronym">RAID-Z</acronym>, a variation on standard
		    <acronym class="acronym">RAID-5</acronym> that offers better
		    distribution of parity and eliminates the
		    <span class="quote">&#8220;<span class="quote"><acronym class="acronym">RAID-5</acronym> write
		    hole</span>&#8221;</span> in which the data and parity
		    information become inconsistent after an
		    unexpected restart.  <acronym class="acronym">ZFS</acronym>
		    supports three levels of <acronym class="acronym">RAID-Z</acronym>
		    which provide varying levels of redundancy in
		    exchange for decreasing levels of usable storage.
		    The types are named <acronym class="acronym">RAID-Z1</acronym>
		    through <acronym class="acronym">RAID-Z3</acronym> based on the
		    number of parity devices in the array and the
		    number of disks which can fail while the pool
		    remains operational.</p><p>In a <acronym class="acronym">RAID-Z1</acronym> configuration
		    with four disks, each 1 TB, usable storage is
		    3 TB and the pool will still be able to
		    operate in degraded mode with one faulted disk.
		    If an additional disk goes offline before the
		    faulted disk is replaced and resilvered, all data
		    in the pool can be lost.</p><p>In a <acronym class="acronym">RAID-Z3</acronym> configuration
		    with eight disks of 1 TB, the volume will
		    provide 5 TB of usable space and still be
		    able to operate with three faulted disks.  <span class="trademark">Sun</span>&#8482;
		    recommends no more than nine disks in a single
		    vdev.  If the configuration has more disks, it is
		    recommended to divide them into separate vdevs and
		    the pool data will be striped across them.</p><p>A configuration of two
		    <acronym class="acronym">RAID-Z2</acronym> vdevs consisting of 8
		    disks each would create something similar to a
		    <acronym class="acronym">RAID-60</acronym> array.  A
		    <acronym class="acronym">RAID-Z</acronym> group's storage capacity
		    is approximately the size of the smallest disk
		    multiplied by the number of non-parity disks.
		    Four 1 TB disks in <acronym class="acronym">RAID-Z1</acronym>
		    has an effective size of approximately 3 TB,
		    and an array of eight 1 TB disks in
		    <acronym class="acronym">RAID-Z3</acronym> will yield 5 TB of
		    usable space.</p></li><li class="listitem"><p><a id="zfs-term-vdev-spare"></a><span class="emphasis"><em>Spare</em></span>
		    - <acronym class="acronym">ZFS</acronym> has a special pseudo-vdev
		    type for keeping track of available hot spares.
		    Note that installed hot spares are not deployed
		    automatically; they must manually be configured to
		    replace the failed device using
		    <code class="command">zfs replace</code>.</p></li><li class="listitem"><p><a id="zfs-term-vdev-log"></a><span class="emphasis"><em>Log</em></span>
		    - <acronym class="acronym">ZFS</acronym> Log Devices, also known
		    as <acronym class="acronym">ZFS</acronym> Intent Log (<a class="link" href="zfs-term.html#zfs-term-zil"><acronym class="acronym">ZIL</acronym></a>)
		    move the intent log from the regular pool devices
		    to a dedicated device, typically an
		    <acronym class="acronym">SSD</acronym>.  Having a dedicated log
		    device can significantly improve the performance
		    of applications with a high volume of synchronous
		    writes, especially databases.  Log devices can be
		    mirrored, but <acronym class="acronym">RAID-Z</acronym> is not
		    supported.  If multiple log devices are used,
		    writes will be load balanced across them.</p></li><li class="listitem"><p><a id="zfs-term-vdev-cache"></a><span class="emphasis"><em>Cache</em></span>
		    - Adding a cache vdev to a pool will add the
		    storage of the cache to the <a class="link" href="zfs-term.html#zfs-term-l2arc"><acronym class="acronym">L2ARC</acronym></a>.
		    Cache devices cannot be mirrored.  Since a cache
		    device only stores additional copies of existing
		    data, there is no risk of data loss.</p></li></ul></div></td></tr><tr><td valign="top"><a id="zfs-term-txg"></a>Transaction Group
	      (<acronym class="acronym">TXG</acronym>)</td><td valign="top">Transaction Groups are the way changed blocks are
	      grouped together and eventually written to the pool.
	      Transaction groups are the atomic unit that
	      <acronym class="acronym">ZFS</acronym> uses to assert consistency.  Each
	      transaction group is assigned a unique 64-bit
	      consecutive identifier.  There can be up to three active
	      transaction groups at a time, one in each of these three
	      states:

	      <div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><span class="emphasis"><em>Open</em></span> - When a new
		    transaction group is created, it is in the open
		    state, and accepts new writes.  There is always
		    a transaction group in the open state, however the
		    transaction group may refuse new writes if it has
		    reached a limit.  Once the open transaction group
		    has reached a limit, or the <a class="link" href="zfs-advanced.html#zfs-advanced-tuning-txg-timeout"><code class="varname">vfs.zfs.txg.timeout</code></a>
		    has been reached, the transaction group advances
		    to the next state.</p></li><li class="listitem"><p><span class="emphasis"><em>Quiescing</em></span> - A short state
		    that allows any pending operations to finish while
		    not blocking the creation of a new open
		    transaction group.  Once all of the transactions
		    in the group have completed, the transaction group
		    advances to the final state.</p></li><li class="listitem"><p><span class="emphasis"><em>Syncing</em></span> - All of the data
		    in the transaction group is written to stable
		    storage.  This process will in turn modify other
		    data, such as metadata and space maps, that will
		    also need to be written to stable storage.  The
		    process of syncing involves multiple passes.  The
		    first, all of the changed data blocks, is the
		    biggest, followed by the metadata, which may take
		    multiple passes to complete.  Since allocating
		    space for the data blocks generates new metadata,
		    the syncing state cannot finish until a pass
		    completes that does not allocate any additional
		    space.  The syncing state is also where
		    <span class="emphasis"><em>synctasks</em></span> are completed.
		    Synctasks are administrative operations, such as
		    creating or destroying snapshots and datasets,
		    that modify the uberblock are completed.  Once the
		    sync state is complete, the transaction group in
		    the quiescing state is advanced to the syncing
		    state.</p></li></ul></div>

	      All administrative functions, such as <a class="link" href="zfs-term.html#zfs-term-snapshot"><code class="command">snapshot</code></a>
	      are written as part of the transaction group.  When a
	      synctask is created, it is added to the currently open
	      transaction group, and that group is advanced as quickly
	      as possible to the syncing state to reduce the
	      latency of administrative commands.</td></tr><tr><td valign="top"><a id="zfs-term-arc"></a>Adaptive Replacement
	      Cache (<acronym class="acronym">ARC</acronym>)</td><td valign="top"><acronym class="acronym">ZFS</acronym> uses an Adaptive Replacement
	      Cache (<acronym class="acronym">ARC</acronym>), rather than a more
	      traditional Least Recently Used (<acronym class="acronym">LRU</acronym>)
	      cache.  An <acronym class="acronym">LRU</acronym> cache is a simple list
	      of items in the cache, sorted by when each object was
	      most recently used.  New items are added to the top of
	      the list.  When the cache is full, items from the
	      bottom of the list are evicted to make room for more
	      active objects.  An <acronym class="acronym">ARC</acronym> consists of
	      four lists; the Most Recently Used
	      (<acronym class="acronym">MRU</acronym>) and Most Frequently Used
	      (<acronym class="acronym">MFU</acronym>) objects, plus a ghost list for
	      each.  These ghost lists track recently evicted objects
	      to prevent them from being added back to the cache.
	      This increases the cache hit ratio by avoiding objects
	      that have a history of only being used occasionally.
	      Another advantage of using both an
	      <acronym class="acronym">MRU</acronym> and <acronym class="acronym">MFU</acronym> is
	      that scanning an entire file system would normally evict
	      all data from an <acronym class="acronym">MRU</acronym> or
	      <acronym class="acronym">LRU</acronym> cache in favor of this freshly
	      accessed content.  With <acronym class="acronym">ZFS</acronym>, there is
	      also an <acronym class="acronym">MFU</acronym> that only tracks the most
	      frequently used objects, and the cache of the most
	      commonly accessed blocks remains.</td></tr><tr><td valign="top"><a id="zfs-term-l2arc"></a><acronym class="acronym">L2ARC</acronym></td><td valign="top"><acronym class="acronym">L2ARC</acronym> is the second level
	      of the <acronym class="acronym">ZFS</acronym> caching system.  The
	      primary <acronym class="acronym">ARC</acronym> is stored in
	      <acronym class="acronym">RAM</acronym>.  Since the amount of
	      available <acronym class="acronym">RAM</acronym> is often limited,
	      <acronym class="acronym">ZFS</acronym> can also use
	      <a class="link" href="zfs-term.html#zfs-term-vdev-cache">cache vdevs</a>.
	      Solid State Disks (<acronym class="acronym">SSD</acronym>s) are often
	      used as these cache devices due to their higher speed
	      and lower latency compared to traditional spinning
	      disks.  <acronym class="acronym">L2ARC</acronym> is entirely optional,
	      but having one will significantly increase read speeds
	      for files that are cached on the <acronym class="acronym">SSD</acronym>
	      instead of having to be read from the regular disks.
	      <acronym class="acronym">L2ARC</acronym> can also speed up <a class="link" href="zfs-term.html#zfs-term-deduplication">deduplication</a>
	      because a <acronym class="acronym">DDT</acronym> that does not fit in
	      <acronym class="acronym">RAM</acronym> but does fit in the
	      <acronym class="acronym">L2ARC</acronym> will be much faster than a
	      <acronym class="acronym">DDT</acronym> that must be read from disk.  The
	      rate at which data is added to the cache devices is
	      limited to prevent prematurely wearing out
	      <acronym class="acronym">SSD</acronym>s with too many writes.  Until the
	      cache is full (the first block has been evicted to make
	      room), writing to the <acronym class="acronym">L2ARC</acronym> is
	      limited to the sum of the write limit and the boost
	      limit, and afterwards limited to the write limit.  A
	      pair of <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=sysctl&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">sysctl</span>(8)</span></a> values control these rate limits.
	      <a class="link" href="zfs-advanced.html#zfs-advanced-tuning-l2arc_write_max"><code class="varname">vfs.zfs.l2arc_write_max</code></a>
	      controls how many bytes are written to the cache per
	      second, while <a class="link" href="zfs-advanced.html#zfs-advanced-tuning-l2arc_write_boost"><code class="varname">vfs.zfs.l2arc_write_boost</code></a>
	      adds to this limit during the
	      <span class="quote">&#8220;<span class="quote">Turbo Warmup Phase</span>&#8221;</span> (Write Boost).</td></tr><tr><td valign="top"><a id="zfs-term-zil"></a><acronym class="acronym">ZIL</acronym></td><td valign="top"><acronym class="acronym">ZIL</acronym> accelerates synchronous
	      transactions by using storage devices like
	      <acronym class="acronym">SSD</acronym>s that are faster than those used
	      in the main storage pool.  When an application requests
	      a synchronous write (a guarantee that the data has been
	      safely stored to disk rather than merely cached to be
	      written later), the data is written to the faster
	      <acronym class="acronym">ZIL</acronym> storage, then later flushed out
	      to the regular disks.  This greatly reduces latency and
	      improves performance.  Only synchronous workloads like
	      databases will benefit from a <acronym class="acronym">ZIL</acronym>.
	      Regular asynchronous writes such as copying files will
	      not use the <acronym class="acronym">ZIL</acronym> at all.</td></tr><tr><td valign="top"><a id="zfs-term-cow"></a>Copy-On-Write</td><td valign="top">Unlike a traditional file system, when data is
	      overwritten on <acronym class="acronym">ZFS</acronym>, the new data is
	      written to a different block rather than overwriting the
	      old data in place.  Only when this write is complete is
	      the metadata then updated to point to the new location.
	      In the event of a shorn write (a system crash or power
	      loss in the middle of writing a file), the entire
	      original contents of the file are still available and
	      the incomplete write is discarded.  This also means that
	      <acronym class="acronym">ZFS</acronym> does not require a <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">fsck</span>(8)</span></a>
	      after an unexpected shutdown.</td></tr><tr><td valign="top"><a id="zfs-term-dataset"></a>Dataset</td><td valign="top"><span class="emphasis"><em>Dataset</em></span> is the generic term
	      for a <acronym class="acronym">ZFS</acronym> file system, volume,
	      snapshot or clone.  Each dataset has a unique name in
	      the format
	      <em class="replaceable"><code>poolname/path@snapshot</code></em>.
	      The root of the pool is technically a dataset as well.
	      Child datasets are named hierarchically like
	      directories.  For example,
	      <em class="replaceable"><code>mypool/home</code></em>, the home
	      dataset, is a child of <em class="replaceable"><code>mypool</code></em>
	      and inherits properties from it.  This can be expanded
	      further by creating
	      <em class="replaceable"><code>mypool/home/user</code></em>.  This
	      grandchild dataset will inherit properties from the
	      parent and grandparent.  Properties on a child can be
	      set to override the defaults inherited from the parents
	      and grandparents.  Administration of datasets and their
	      children can be
	      <a class="link" href="zfs-zfs-allow.html" title="19.5. Delegated Administration">delegated</a>.</td></tr><tr><td valign="top"><a id="zfs-term-filesystem"></a>File system</td><td valign="top">A <acronym class="acronym">ZFS</acronym> dataset is most often used
	      as a file system.  Like most other file systems, a
	      <acronym class="acronym">ZFS</acronym> file system is mounted somewhere
	      in the systems directory hierarchy and contains files
	      and directories of its own with permissions, flags, and
	      other metadata.</td></tr><tr><td valign="top"><a id="zfs-term-volume"></a>Volume</td><td valign="top">In additional to regular file system datasets,
	      <acronym class="acronym">ZFS</acronym> can also create volumes, which
	      are block devices.  Volumes have many of the same
	      features, including copy-on-write, snapshots, clones,
	      and checksumming.  Volumes can be useful for running
	      other file system formats on top of
	      <acronym class="acronym">ZFS</acronym>, such as <acronym class="acronym">UFS</acronym>
	      virtualization, or exporting <acronym class="acronym">iSCSI</acronym>
	      extents.</td></tr><tr><td valign="top"><a id="zfs-term-snapshot"></a>Snapshot</td><td valign="top">The
	      <a class="link" href="zfs-term.html#zfs-term-cow">copy-on-write</a>
	      (<acronym class="acronym">COW</acronym>) design of
	      <acronym class="acronym">ZFS</acronym> allows for nearly instantaneous,
	      consistent snapshots with arbitrary names.  After taking
	      a snapshot of a dataset, or a recursive snapshot of a
	      parent dataset that will include all child datasets, new
	      data is written to new blocks, but the old blocks are
	      not reclaimed as free space.  The snapshot contains
	      the original version of the file system, and the live
	      file system contains any changes made since the snapshot
	      was taken.  No additional space is used.  As new data is
	      written to the live file system, new blocks are
	      allocated to store this data.  The apparent size of the
	      snapshot will grow as the blocks are no longer used in
	      the live file system, but only in the snapshot.  These
	      snapshots can be mounted read only to allow for the
	      recovery of previous versions of files.  It is also
	      possible to
	      <a class="link" href="zfs-zfs.html#zfs-zfs-snapshot" title="19.4.5. Managing Snapshots">rollback</a> a live
	      file system to a specific snapshot, undoing any changes
	      that took place after the snapshot was taken.  Each
	      block in the pool has a reference counter which keeps
	      track of how many snapshots, clones, datasets, or
	      volumes make use of that block.  As files and snapshots
	      are deleted, the reference count is decremented.  When a
	      block is no longer referenced, it is reclaimed as free
	      space.  Snapshots can also be marked with a
	      <a class="link" href="zfs-zfs.html#zfs-zfs-snapshot" title="19.4.5. Managing Snapshots">hold</a>.  When a
	      snapshot is held, any attempt to destroy it will return
	      an <code class="literal">EBUSY</code> error.  Each snapshot can
	      have multiple holds, each with a unique name.  The
	      <a class="link" href="zfs-zfs.html#zfs-zfs-snapshot" title="19.4.5. Managing Snapshots">release</a> command
	      removes the hold so the snapshot can deleted.  Snapshots
	      can be taken on volumes, but they can only be cloned or
	      rolled back, not mounted independently.</td></tr><tr><td valign="top"><a id="zfs-term-clone"></a>Clone</td><td valign="top">Snapshots can also be cloned.  A clone is a
	      writable version of a snapshot, allowing the file system
	      to be forked as a new dataset.  As with a snapshot, a
	      clone initially consumes no additional space.  As
	      new data is written to a clone and new blocks are
	      allocated, the apparent size of the clone grows.  When
	      blocks are overwritten in the cloned file system or
	      volume, the reference count on the previous block is
	      decremented.  The snapshot upon which a clone is based
	      cannot be deleted because the clone depends on it.  The
	      snapshot is the parent, and the clone is the child.
	      Clones can be <span class="emphasis"><em>promoted</em></span>, reversing
	      this dependency and making the clone the parent and the
	      previous parent the child.  This operation requires no
	      additional space.  Because the amount of space used by
	      the parent and child is reversed, existing quotas and
	      reservations might be affected.</td></tr><tr><td valign="top"><a id="zfs-term-checksum"></a>Checksum</td><td valign="top">Every block that is allocated is also checksummed.
	      The checksum algorithm used is a per-dataset property,
	      see <a class="link" href="zfs-zfs.html#zfs-zfs-set" title="19.4.4. Setting Dataset Properties"><code class="command">set</code></a>.
	      The checksum of each block is transparently validated as
	      it is read, allowing <acronym class="acronym">ZFS</acronym> to detect
	      silent corruption.  If the data that is read does not
	      match the expected checksum, <acronym class="acronym">ZFS</acronym> will
	      attempt to recover the data from any available
	      redundancy, like mirrors or <acronym class="acronym">RAID-Z</acronym>).
	      Validation of all checksums can be triggered with <a class="link" href="zfs-term.html#zfs-term-scrub"><code class="command">scrub</code></a>.
	      Checksum algorithms include:

	      <div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><code class="literal">fletcher2</code></p></li><li class="listitem"><p><code class="literal">fletcher4</code></p></li><li class="listitem"><p><code class="literal">sha256</code></p></li></ul></div>

	      The <code class="literal">fletcher</code> algorithms are faster,
	      but <code class="literal">sha256</code> is a strong cryptographic
	      hash and has a much lower chance of collisions at the
	      cost of some performance.  Checksums can be disabled,
	      but it is not recommended.</td></tr><tr><td valign="top"><a id="zfs-term-compression"></a>Compression</td><td valign="top">Each dataset has a compression property, which
	      defaults to off.  This property can be set to one of a
	      number of compression algorithms.  This will cause all
	      new data that is written to the dataset to be
	      compressed.  Beyond a reduction in space used, read and
	      write throughput often increases because fewer blocks
	      are read or written.

	      <div class="itemizedlist"><ul class="itemizedlist" style="list-style-type: disc; "><li class="listitem"><p><a id="zfs-term-compression-lz4"></a><span class="emphasis"><em><acronym class="acronym">LZ4</acronym></em></span> -
		    Added in <acronym class="acronym">ZFS</acronym> pool version
		    5000 (feature flags), <acronym class="acronym">LZ4</acronym> is
		    now the recommended compression algorithm.
		    <acronym class="acronym">LZ4</acronym> compresses approximately
		    50% faster than <acronym class="acronym">LZJB</acronym> when
		    operating on compressible data, and is over three
		    times faster when operating on uncompressible
		    data.  <acronym class="acronym">LZ4</acronym> also decompresses
		    approximately 80% faster than
		    <acronym class="acronym">LZJB</acronym>.  On modern
		    <acronym class="acronym">CPU</acronym>s, <acronym class="acronym">LZ4</acronym>
		    can often compress at over 500 MB/s, and
		    decompress at over 1.5 GB/s (per single CPU
		    core).</p></li><li class="listitem"><p><a id="zfs-term-compression-lzjb"></a><span class="emphasis"><em><acronym class="acronym">LZJB</acronym></em></span> -
		    The default compression algorithm.  Created by
		    Jeff Bonwick (one of the original creators of
		    <acronym class="acronym">ZFS</acronym>).  <acronym class="acronym">LZJB</acronym>
		    offers good compression with less
		    <acronym class="acronym">CPU</acronym> overhead compared to
		    <acronym class="acronym">GZIP</acronym>.  In the future, the
		    default compression algorithm will likely change
		    to <acronym class="acronym">LZ4</acronym>.</p></li><li class="listitem"><p><a id="zfs-term-compression-gzip"></a><span class="emphasis"><em><acronym class="acronym">GZIP</acronym></em></span> -
		    A popular stream compression algorithm available
		    in <acronym class="acronym">ZFS</acronym>.  One of the main
		    advantages of using <acronym class="acronym">GZIP</acronym> is its
		    configurable level of compression.  When setting
		    the <code class="literal">compress</code> property, the
		    administrator can choose the level of compression,
		    ranging from <code class="literal">gzip1</code>, the lowest
		    level of compression, to <code class="literal">gzip9</code>,
		    the highest level of compression.  This gives the
		    administrator control over how much
		    <acronym class="acronym">CPU</acronym> time to trade for saved
		    disk space.</p></li><li class="listitem"><p><a id="zfs-term-compression-zle"></a><span class="emphasis"><em><acronym class="acronym">ZLE</acronym></em></span> -
		    Zero Length Encoding is a special compression
		    algorithm that only compresses continuous runs of
		    zeros.  This compression algorithm is only useful
		    when the dataset contains large blocks of
		    zeros.</p></li></ul></div></td></tr><tr><td valign="top"><a id="zfs-term-copies"></a>Copies</td><td valign="top">When set to a value greater than 1, the
	      <code class="literal">copies</code> property instructs
	      <acronym class="acronym">ZFS</acronym> to maintain multiple copies of
	      each block in the
	      <a class="link" href="zfs-term.html#zfs-term-filesystem">File System</a>
	      or
	      <a class="link" href="zfs-term.html#zfs-term-volume">Volume</a>.  Setting
	      this property on important datasets provides additional
	      redundancy from which to recover a block that does not
	      match its checksum.  In pools without redundancy, the
	      copies feature is the only form of redundancy.  The
	      copies feature can recover from a single bad sector or
	      other forms of minor corruption, but it does not protect
	      the pool from the loss of an entire disk.</td></tr><tr><td valign="top"><a id="zfs-term-deduplication"></a>Deduplication</td><td valign="top">Checksums make it possible to detect duplicate
	      blocks of data as they are written.  With deduplication,
	      the reference count of an existing, identical block is
	      increased, saving storage space.  To detect duplicate
	      blocks, a deduplication table (<acronym class="acronym">DDT</acronym>)
	      is kept in memory.  The table contains a list of unique
	      checksums, the location of those blocks, and a reference
	      count.  When new data is written, the checksum is
	      calculated and compared to the list.  If a match is
	      found, the existing block is used.  The
	      <acronym class="acronym">SHA256</acronym> checksum algorithm is used
	      with deduplication to provide a secure cryptographic
	      hash.  Deduplication is tunable.  If
	      <code class="literal">dedup</code> is <code class="literal">on</code>, then
	      a matching checksum is assumed to mean that the data is
	      identical.  If <code class="literal">dedup</code> is set to
	      <code class="literal">verify</code>, then the data in the two
	      blocks will be checked byte-for-byte to ensure it is
	      actually identical.  If the data is not identical, the
	      hash collision will be noted and the two blocks will be
	      stored separately.  Because <acronym class="acronym">DDT</acronym> must
	      store the hash of each unique block, it consumes a very
	      large amount of memory.  A general rule of thumb is
	      5-6 GB of ram per 1 TB of deduplicated data).
	      In situations where it is not practical to have enough
	      <acronym class="acronym">RAM</acronym> to keep the entire
	      <acronym class="acronym">DDT</acronym> in memory, performance will
	      suffer greatly as the <acronym class="acronym">DDT</acronym> must be
	      read from disk before each new block is written.
	      Deduplication can use <acronym class="acronym">L2ARC</acronym> to store
	      the <acronym class="acronym">DDT</acronym>, providing a middle ground
	      between fast system memory and slower disks.  Consider
	      using compression instead, which often provides nearly
	      as much space savings without the additional memory
	      requirement.</td></tr><tr><td valign="top"><a id="zfs-term-scrub"></a>Scrub</td><td valign="top">Instead of a consistency check like <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=fsck&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">fsck</span>(8)</span></a>,
	      <acronym class="acronym">ZFS</acronym> has <code class="command">scrub</code>.
	      <code class="command">scrub</code> reads all data blocks stored on
	      the pool and verifies their checksums against the known
	      good checksums stored in the metadata.  A periodic check
	      of all the data stored on the pool ensures the recovery
	      of any corrupted blocks before they are needed.  A scrub
	      is not required after an unclean shutdown, but is
	      recommended at least once every three months.  The
	      checksum of each block is verified as blocks are read
	      during normal use, but a scrub makes certain that even
	      infrequently used blocks are checked for silent
	      corruption.  Data security is improved, especially in
	      archival storage situations.  The relative priority of
	      <code class="command">scrub</code> can be adjusted with <a class="link" href="zfs-advanced.html#zfs-advanced-tuning-scrub_delay"><code class="varname">vfs.zfs.scrub_delay</code></a>
	      to prevent the scrub from degrading the performance of
	      other workloads on the pool.</td></tr><tr><td valign="top"><a id="zfs-term-quota"></a>Dataset Quota</td><td valign="top"><acronym class="acronym">ZFS</acronym> provides very fast and
	      accurate dataset, user, and group space accounting in
	      addition to quotas and space reservations.  This gives
	      the administrator fine grained control over how space is
	      allocated and allows space to be reserved for critical
	      file systems.

	      <p><acronym class="acronym">ZFS</acronym> supports different types of
		quotas: the dataset quota, the <a class="link" href="zfs-term.html#zfs-term-refquota">reference
		  quota (<acronym class="acronym">refquota</acronym>)</a>, the
		<a class="link" href="zfs-term.html#zfs-term-userquota">user
		  quota</a>, and the
		<a class="link" href="zfs-term.html#zfs-term-groupquota">group
		  quota</a>.</p>

	      <p>Quotas limit the amount of space that a dataset
		and all of its descendants, including snapshots of the
		dataset, child datasets, and the snapshots of those
		datasets, can consume.</p>

	      <div xmlns="" class="note"><h3 class="admontitle">Note: </h3><p xmlns="http://www.w3.org/1999/xhtml">Quotas cannot be set on volumes, as the
		  <code class="literal">volsize</code> property acts as an
		  implicit quota.</p></div></td></tr><tr><td valign="top"><a id="zfs-term-refquota"></a>Reference
	      Quota</td><td valign="top">A reference quota limits the amount of space a
	      dataset can consume by enforcing a hard limit.  However,
	      this hard limit includes only space that the dataset
	      references and does not include space used by
	      descendants, such as file systems or snapshots.</td></tr><tr><td valign="top"><a id="zfs-term-userquota"></a>User
	      Quota</td><td valign="top">User quotas are useful to limit the amount of space
	      that can be used by the specified user.</td></tr><tr><td valign="top"><a id="zfs-term-groupquota"></a>Group
	      Quota</td><td valign="top">The group quota limits the amount of space that a
	      specified group can consume.</td></tr><tr><td valign="top"><a id="zfs-term-reservation"></a>Dataset
	      Reservation</td><td valign="top">The <code class="literal">reservation</code> property makes
	      it possible to guarantee a minimum amount of space for a
	      specific dataset and its descendants.  If a 10 GB
	      reservation is set on
	      <code class="filename">storage/home/bob</code>, and another
	      dataset tries to use all of the free space, at least
	      10 GB of space is reserved for this dataset.  If a
	      snapshot is taken of
	      <code class="filename">storage/home/bob</code>, the space used by
	      that snapshot is counted against the reservation.  The
	      <a class="link" href="zfs-term.html#zfs-term-refreservation"><code class="literal">refreservation</code></a>
	      property works in a similar way, but it
	      <span class="emphasis"><em>excludes</em></span> descendants like
	      snapshots.

	      <p>Reservations of any sort are useful in many
		situations, such as planning and testing the
		suitability of disk space allocation in a new system,
		or ensuring that enough space is available on file
		systems for audio logs or system recovery procedures
		and files.</p>
	    </td></tr><tr><td valign="top"><a id="zfs-term-refreservation"></a>Reference
	      Reservation</td><td valign="top">The <code class="literal">refreservation</code> property
	      makes it possible to guarantee a minimum amount of
	      space for the use of a specific dataset
	      <span class="emphasis"><em>excluding</em></span> its descendants.  This
	      means that if a 10 GB reservation is set on
	      <code class="filename">storage/home/bob</code>, and another
	      dataset tries to use all of the free space, at least
	      10 GB of space is reserved for this dataset.  In
	      contrast to a regular
	      <a class="link" href="zfs-term.html#zfs-term-reservation">reservation</a>,
	      space used by snapshots and descendant datasets is not
	      counted against the reservation.  For example, if a
	      snapshot is taken of
	      <code class="filename">storage/home/bob</code>, enough disk space
	      must exist outside of the
	      <code class="literal">refreservation</code> amount for the
	      operation to succeed.  Descendants of the main data set
	      are not counted in the <code class="literal">refreservation</code>
	      amount and so do not encroach on the space set.</td></tr><tr><td valign="top"><a id="zfs-term-resilver"></a>Resilver</td><td valign="top">When a disk fails and is replaced, the new disk
	      must be filled with the data that was lost.  The process
	      of using the parity information distributed across the
	      remaining drives to calculate and write the missing data
	      to the new drive is called
	      <span class="emphasis"><em>resilvering</em></span>.</td></tr><tr><td valign="top"><a id="zfs-term-online"></a>Online</td><td valign="top">A pool or vdev in the <code class="literal">Online</code>
	      state has all of its member devices connected and fully
	      operational.  Individual devices in the
	      <code class="literal">Online</code> state are functioning
	      normally.</td></tr><tr><td valign="top"><a id="zfs-term-offline"></a>Offline</td><td valign="top">Individual devices can be put in an
	      <code class="literal">Offline</code> state by the administrator if
	      there is sufficient redundancy to avoid putting the pool
	      or vdev into a
	      <a class="link" href="zfs-term.html#zfs-term-faulted">Faulted</a> state.
	      An administrator may choose to offline a disk in
	      preparation for replacing it, or to make it easier to
	      identify.</td></tr><tr><td valign="top"><a id="zfs-term-degraded"></a>Degraded</td><td valign="top">A pool or vdev in the <code class="literal">Degraded</code>
	      state has one or more disks that have been disconnected
	      or have failed.  The pool is still usable, but if
	      additional devices fail, the pool could become
	      unrecoverable.  Reconnecting the missing devices or
	      replacing the failed disks will return the pool to an
	      <a class="link" href="zfs-term.html#zfs-term-online">Online</a> state
	      after the reconnected or new device has completed the
	      <a class="link" href="zfs-term.html#zfs-term-resilver">Resilver</a>
	      process.</td></tr><tr><td valign="top"><a id="zfs-term-faulted"></a>Faulted</td><td valign="top">A pool or vdev in the <code class="literal">Faulted</code>
	      state is no longer operational.  The data on it can no
	      longer be accessed.  A pool or vdev enters the
	      <code class="literal">Faulted</code> state when the number of
	      missing or failed devices exceeds the level of
	      redundancy in the vdev.  If missing devices can be
	      reconnected, the pool will return to a
	      <a class="link" href="zfs-term.html#zfs-term-online">Online</a> state.  If
	      there is insufficient redundancy to compensate for the
	      number of failed disks, then the contents of the pool
	      are lost and must be restored from backups.</td></tr></tbody></table></div></div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="zfs-links.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="zfs.html">Up</a></td><td width="40%" align="right"> <a accesskey="n" href="filesystems.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">19.7. Additional Resources </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> Chapter 20. Other File Systems</td></tr></table></div><p xmlns="" align="center"><small>All FreeBSD documents are available for download
    at <a href="https://download.freebsd.org/ftp/doc/">https://download.freebsd.org/ftp/doc/</a></small></p><p xmlns="" align="center"><small>Questions that are not answered by the
    <a href="https://www.FreeBSD.org/docs.html">documentation</a> may be
    sent to &lt;<a href="mailto:freebsd-questions@FreeBSD.org">freebsd-questions@FreeBSD.org</a>&gt;.<br />
    Send questions about this document to &lt;<a href="mailto:freebsd-doc@FreeBSD.org">freebsd-doc@FreeBSD.org</a>&gt;.</small></p></body></html>