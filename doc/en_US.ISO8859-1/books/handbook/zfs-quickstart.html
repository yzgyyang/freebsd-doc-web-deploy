<?xml version="1.0" encoding="iso-8859-1" standalone="no"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" /><title>19.2. Quick Start Guide</title><link rel="stylesheet" type="text/css" href="docbook.css" /><link rev="made" href="mailto:doc@FreeBSD.org" /><meta name="generator" content="DocBook XSL Stylesheets V1.79.1" /><link rel="home" href="index.html" title="FreeBSD Handbook" /><link rel="up" href="zfs.html" title="Chapter 19. The Z File System (ZFS)" /><link rel="prev" href="zfs.html" title="Chapter 19. The Z File System (ZFS)" /><link rel="next" href="zfs-zpool.html" title="19.3. zpool Administration" /><link rel="copyright" href="legalnotice.html" title="Copyright" /><script xmlns="" type="text/javascript" src="/layout/js/google.js"></script></head><body><div class="navheader"><table width="100%" summary="Navigation header"><tr><th colspan="3" align="center">19.2. Quick Start Guide</th></tr><tr><td width="20%" align="left"><a accesskey="p" href="zfs.html">Prev</a> </td><th width="60%" align="center">Chapter 19. The Z File System (<acronym class="acronym">ZFS</acronym>)</th><td width="20%" align="right"> <a accesskey="n" href="zfs-zpool.html">Next</a></td></tr></table><hr /></div><div class="sect1"><div xmlns="" class="titlepage"><div><div><h2 xmlns="http://www.w3.org/1999/xhtml" class="title" style="clear: both"><a id="zfs-quickstart"></a>19.2. Quick Start Guide</h2></div></div></div><p>There is a startup mechanism that allows FreeBSD to mount
      <acronym class="acronym">ZFS</acronym> pools during system initialization.  To
      enable it, add this line to
      <code class="filename">/etc/rc.conf</code>:</p><pre class="programlisting">zfs_enable="YES"</pre><p>Then start the service:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>service zfs start</code></strong></pre><p>The examples in this section assume three
      <acronym class="acronym">SCSI</acronym> disks with the device names
      <code class="filename"><em class="replaceable"><code>da0</code></em></code>,
      <code class="filename"><em class="replaceable"><code>da1</code></em></code>, and
      <code class="filename"><em class="replaceable"><code>da2</code></em></code>.  Users
      of <acronym class="acronym">SATA</acronym> hardware should instead use
      <code class="filename"><em class="replaceable"><code>ada</code></em></code> device
      names.</p><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-quickstart-single-disk-pool"></a>19.2.1. Single Disk Pool</h3></div></div></div><p>To create a simple, non-redundant pool using a single
	disk device:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool create <em class="replaceable"><code>example</code></em> <em class="replaceable"><code>/dev/da0</code></em></code></strong></pre><p>To view the new pool, review the output of
	<code class="command">df</code>:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>df</code></strong>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235230  1628718    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032846 48737598     2%    /usr
example      17547136       0 17547136     0%    /example</pre><p>This output shows that the <code class="literal">example</code> pool
	has been created and mounted.  It is now accessible as a file
	system.  Files can be created on it and users can browse
	it:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>cd /example</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>ls</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>touch testfile</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>ls -al</code></strong>
total 4
drwxr-xr-x   2 root  wheel    3 Aug 29 23:15 .
drwxr-xr-x  21 root  wheel  512 Aug 29 23:12 ..
-rw-r--r--   1 root  wheel    0 Aug 29 23:15 testfile</pre><p>However, this pool is not taking advantage of any
	<acronym class="acronym">ZFS</acronym> features.  To create a dataset on this
	pool with compression enabled:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zfs create example/compressed</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zfs set compression=gzip example/compressed</code></strong></pre><p>The <code class="literal">example/compressed</code> dataset is now a
	<acronym class="acronym">ZFS</acronym> compressed file system.  Try copying
	some large files to
	<code class="filename">/example/compressed</code>.</p><p>Compression can be disabled with:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zfs set compression=off example/compressed</code></strong></pre><p>To unmount a file system, use
	<code class="command">zfs umount</code> and then verify with
	<code class="command">df</code>:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zfs umount example/compressed</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>df</code></strong>
Filesystem  1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a   2026030  235232  1628716    13%    /
devfs               1       1        0   100%    /dev
/dev/ad0s1d  54098308 1032864 48737580     2%    /usr
example      17547008       0 17547008     0%    /example</pre><p>To re-mount the file system to make it accessible again,
	use <code class="command">zfs mount</code> and verify with
	<code class="command">df</code>:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zfs mount example/compressed</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>df</code></strong>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed</pre><p>The pool and file system may also be observed by viewing
	the output from <code class="command">mount</code>:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>mount</code></strong>
/dev/ad0s1a on / (ufs, local)
devfs on /dev (devfs, local)
/dev/ad0s1d on /usr (ufs, local, soft-updates)
example on /example (zfs, local)
example/compressed on /example/compressed (zfs, local)</pre><p>After creation, <acronym class="acronym">ZFS</acronym> datasets can be
	used like any file systems.  However, many other features are
	available which can be set on a per-dataset basis.  In the
	example below, a new file system called
	<code class="literal">data</code> is created.  Important files will be
	stored here, so it is configured to keep two copies of each
	data block:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zfs create example/data</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zfs set copies=2 example/data</code></strong></pre><p>It is now possible to see the data and space utilization
	by issuing <code class="command">df</code>:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>df</code></strong>
Filesystem         1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a          2026030  235234  1628714    13%    /
devfs                      1       1        0   100%    /dev
/dev/ad0s1d         54098308 1032864 48737580     2%    /usr
example             17547008       0 17547008     0%    /example
example/compressed  17547008       0 17547008     0%    /example/compressed
example/data        17547008       0 17547008     0%    /example/data</pre><p>Notice that each file system on the pool has the same
	amount of available space.  This is the reason for using
	<code class="command">df</code> in these examples, to show that the file
	systems use only the amount of space they need and all draw
	from the same pool.  <acronym class="acronym">ZFS</acronym> eliminates
	concepts such as volumes and partitions, and allows multiple
	file systems to occupy the same pool.</p><p>To destroy the file systems and then destroy the pool as
	it is no longer needed:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zfs destroy example/compressed</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zfs destroy example/data</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zpool destroy example</code></strong></pre></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-quickstart-raid-z"></a>19.2.2. RAID-Z</h3></div></div></div><p>Disks fail.  One method of avoiding data loss from disk
	failure is to implement <acronym class="acronym">RAID</acronym>.
	<acronym class="acronym">ZFS</acronym> supports this feature in its pool
	design.  <acronym class="acronym">RAID-Z</acronym> pools require three or more
	disks but provide more usable space than mirrored
	pools.</p><p>This example creates a <acronym class="acronym">RAID-Z</acronym> pool,
	specifying the disks to add to the pool:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool create storage raidz da0 da1 da2</code></strong></pre><div xmlns="" class="note"><h3 class="admontitle">Note: </h3><p xmlns="http://www.w3.org/1999/xhtml"><span class="trademark">Sun</span>&#8482; recommends that the number of devices used in a
	  <acronym class="acronym">RAID</acronym>-Z configuration be between three and
	  nine.  For environments requiring a single pool consisting
	  of 10 disks or more, consider breaking it up into smaller
	  <acronym class="acronym">RAID-Z</acronym> groups.  If only two disks are
	  available and redundancy is a requirement, consider using a
	  <acronym class="acronym">ZFS</acronym> mirror.  Refer to <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=zpool&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">zpool</span>(8)</span></a> for
	  more details.</p></div><p>The previous example created the
	<code class="literal">storage</code> zpool.  This example makes a new
	file system called <code class="literal">home</code> in that
	pool:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zfs create storage/home</code></strong></pre><p>Compression and keeping extra copies of directories
	and files can be enabled:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zfs set copies=2 storage/home</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>zfs set compression=gzip storage/home</code></strong></pre><p>To make this the new home directory for users, copy the
	user data to this directory and create the appropriate
	symbolic links:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>cp -rp /home/* /storage/home</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>rm -rf /home /usr/home</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>ln -s /storage/home /home</code></strong>
<code class="prompt">#</code> <strong class="userinput"><code>ln -s /storage/home /usr/home</code></strong></pre><p>Users data is now stored on the freshly-created
	<code class="filename">/storage/home</code>.  Test by adding a new user
	and logging in as that user.</p><p>Try creating a file system snapshot which can be rolled
	back later:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zfs snapshot storage/home@08-30-08</code></strong></pre><p>Snapshots can only be made of a full file system, not a
	single directory or file.</p><p>The <code class="literal">@</code> character is a delimiter between
	the file system name or the volume name.  If an important
	directory has been accidentally deleted, the file system can
	be backed up, then rolled back to an earlier snapshot when the
	directory still existed:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zfs rollback storage/home@08-30-08</code></strong></pre><p>To list all available snapshots, run
	<code class="command">ls</code> in the file system's
	<code class="filename">.zfs/snapshot</code> directory.  For example, to
	see the previously taken snapshot:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>ls /storage/home/.zfs/snapshot</code></strong></pre><p>It is possible to write a script to perform regular
	snapshots on user data.  However, over time, snapshots can
	consume a great deal of disk space.  The previous snapshot can
	be removed using the command:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zfs destroy storage/home@08-30-08</code></strong></pre><p>After testing, <code class="filename">/storage/home</code> can be
	made the real <code class="filename">/home</code> using this
	command:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zfs set mountpoint=/home storage/home</code></strong></pre><p>Run <code class="command">df</code> and <code class="command">mount</code> to
	confirm that the system now treats the file system as the real
	<code class="filename">/home</code>:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>mount</code></strong>
/dev/ad0s1a on / (ufs, local)
devfs on /dev (devfs, local)
/dev/ad0s1d on /usr (ufs, local, soft-updates)
storage on /storage (zfs, local)
storage/home on /home (zfs, local)
<code class="prompt">#</code> <strong class="userinput"><code>df</code></strong>
Filesystem   1K-blocks    Used    Avail Capacity  Mounted on
/dev/ad0s1a    2026030  235240  1628708    13%    /
devfs                1       1        0   100%    /dev
/dev/ad0s1d   54098308 1032826 48737618     2%    /usr
storage       26320512       0 26320512     0%    /storage
storage/home  26320512       0 26320512     0%    /home</pre><p>This completes the <acronym class="acronym">RAID-Z</acronym>
	configuration.  Daily status updates about the file systems
	created can be generated as part of the nightly
	<a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=periodic&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">periodic</span>(8)</span></a> runs.  Add this line to
	<code class="filename">/etc/periodic.conf</code>:</p><pre class="programlisting">daily_status_zfs_enable="YES"</pre></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-quickstart-recovering-raid-z"></a>19.2.3. Recovering <acronym class="acronym">RAID-Z</acronym></h3></div></div></div><p>Every software <acronym class="acronym">RAID</acronym> has a method of
	monitoring its <code class="literal">state</code>.  The status of
	<acronym class="acronym">RAID-Z</acronym> devices may be viewed with this
	command:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool status -x</code></strong></pre><p>If all pools are
	<a class="link" href="zfs-term.html#zfs-term-online">Online</a> and everything
	is normal, the message shows:</p><pre class="screen">all pools are healthy</pre><p>If there is an issue, perhaps a disk is in the
	<a class="link" href="zfs-term.html#zfs-term-offline">Offline</a> state, the
	pool state will look similar to:</p><pre class="screen">  pool: storage
 state: DEGRADED
status: One or more devices has been taken offline by the administrator.
	Sufficient replicas exist for the pool to continue functioning in a
	degraded state.
action: Online the device using 'zpool online' or replace the device with
	'zpool replace'.
 scrub: none requested
config:

	NAME        STATE     READ WRITE CKSUM
	storage     DEGRADED     0     0     0
	  raidz1    DEGRADED     0     0     0
	    da0     ONLINE       0     0     0
	    da1     OFFLINE      0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</pre><p>This indicates that the device was previously taken
	offline by the administrator with this command:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool offline storage da1</code></strong></pre><p>Now the system can be powered down to replace
	<code class="filename">da1</code>.  When the system is back online,
	the failed disk can replaced in the pool:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool replace storage da1</code></strong></pre><p>From here, the status may be checked again, this time
	without <code class="option">-x</code> so that all pools are
	shown:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool status storage</code></strong>
 pool: storage
 state: ONLINE
 scrub: resilver completed with 0 errors on Sat Aug 30 19:44:11 2008
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</pre><p>In this example, everything is normal.</p></div><div class="sect2"><div xmlns="" class="titlepage"><div><div><h3 xmlns="http://www.w3.org/1999/xhtml" class="title"><a id="zfs-quickstart-data-verification"></a>19.2.4. Data Verification</h3></div></div></div><p><acronym class="acronym">ZFS</acronym> uses checksums to verify the
	integrity of stored data.  These are enabled automatically
	upon creation of file systems.</p><div xmlns="" class="warning"><h3 class="admontitle">Warning: </h3><p xmlns="http://www.w3.org/1999/xhtml">Checksums can be disabled, but it is
	  <span class="emphasis"><em>not</em></span> recommended!  Checksums take very
	  little storage space and provide data integrity.  Many
	  <acronym class="acronym">ZFS</acronym> features will not work properly with
	  checksums disabled.  There is no noticeable performance gain
	  from disabling these checksums.</p></div><p>Checksum verification is known as
	<span class="emphasis"><em>scrubbing</em></span>.  Verify the data integrity of
	the <code class="literal">storage</code> pool with this command:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool scrub storage</code></strong></pre><p>The duration of a scrub depends on the amount of data
	stored.  Larger amounts of data will take proportionally
	longer to verify.  Scrubs are very <acronym class="acronym">I/O</acronym>
	intensive, and only one scrub is allowed to run at a time.
	After the scrub completes, the status can be viewed with
	<code class="command">status</code>:</p><pre class="screen"><code class="prompt">#</code> <strong class="userinput"><code>zpool status storage</code></strong>
 pool: storage
 state: ONLINE
 scrub: scrub completed with 0 errors on Sat Jan 26 19:57:37 2013
config:

	NAME        STATE     READ WRITE CKSUM
	storage     ONLINE       0     0     0
	  raidz1    ONLINE       0     0     0
	    da0     ONLINE       0     0     0
	    da1     ONLINE       0     0     0
	    da2     ONLINE       0     0     0

errors: No known data errors</pre><p>The completion date of the last scrub operation is
	displayed to help track when another scrub is required.
	Routine scrubs help protect data from silent corruption and
	ensure the integrity of the pool.</p><p>Refer to <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=zfs&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">zfs</span>(8)</span></a> and <a class="citerefentry" href="https://www.FreeBSD.org/cgi/man.cgi?query=zpool&amp;sektion=8&amp;manpath=freebsd-release-ports"><span class="citerefentry"><span class="refentrytitle">zpool</span>(8)</span></a> for other
	<acronym class="acronym">ZFS</acronym> options.</p></div></div><div class="navfooter"><hr /><table width="100%" summary="Navigation footer"><tr><td width="40%" align="left"><a accesskey="p" href="zfs.html">Prev</a> </td><td width="20%" align="center"><a accesskey="u" href="zfs.html">Up</a></td><td width="40%" align="right"> <a accesskey="n" href="zfs-zpool.html">Next</a></td></tr><tr><td width="40%" align="left" valign="top">Chapter 19. The Z File System (<acronym class="acronym">ZFS</acronym>) </td><td width="20%" align="center"><a accesskey="h" href="index.html">Home</a></td><td width="40%" align="right" valign="top"> 19.3. <code class="command">zpool</code> Administration</td></tr></table></div><p xmlns="" align="center"><small>All FreeBSD documents are available for download
    at <a href="https://download.freebsd.org/ftp/doc/">https://download.freebsd.org/ftp/doc/</a></small></p><p xmlns="" align="center"><small>Questions that are not answered by the
    <a href="https://www.FreeBSD.org/docs.html">documentation</a> may be
    sent to &lt;<a href="mailto:freebsd-questions@FreeBSD.org">freebsd-questions@FreeBSD.org</a>&gt;.<br />
    Send questions about this document to &lt;<a href="mailto:freebsd-doc@FreeBSD.org">freebsd-doc@FreeBSD.org</a>&gt;.</small></p></body></html>